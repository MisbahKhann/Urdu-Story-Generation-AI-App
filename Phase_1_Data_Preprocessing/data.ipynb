{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c56f87",
   "metadata": {},
   "source": [
    "## attempt 8-Final Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96aa6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ Collecting story links from all pages...\n",
      "\n",
      "๐ Page 1: https://www.urdupoint.com/kids/category/moral-stories.html\n",
      "   โ Found 12 stories (Total: 12)\n",
      "\n",
      "๐ Page 2: https://www.urdupoint.com/kids/category/moral-stories-page2.html\n",
      "   โ Found 12 stories (Total: 24)\n",
      "\n",
      "๐ Page 3: https://www.urdupoint.com/kids/category/moral-stories-page3.html\n",
      "   โ Found 12 stories (Total: 36)\n",
      "\n",
      "๐ Page 4: https://www.urdupoint.com/kids/category/moral-stories-page4.html\n",
      "   โ Found 12 stories (Total: 48)\n",
      "\n",
      "๐ Page 5: https://www.urdupoint.com/kids/category/moral-stories-page5.html\n",
      "   โ๏ธ Page load timeout - skipping page 5\n",
      "\n",
      "๐ Page 6: https://www.urdupoint.com/kids/category/moral-stories-page6.html\n",
      "   โ Found 12 stories (Total: 60)\n",
      "\n",
      "๐ Page 7: https://www.urdupoint.com/kids/category/moral-stories-page7.html\n",
      "   โ Found 12 stories (Total: 72)\n",
      "\n",
      "๐ Page 8: https://www.urdupoint.com/kids/category/moral-stories-page8.html\n",
      "   โ Found 12 stories (Total: 84)\n",
      "\n",
      "๐ Page 9: https://www.urdupoint.com/kids/category/moral-stories-page9.html\n",
      "   โ Found 12 stories (Total: 96)\n",
      "\n",
      "๐ Page 10: https://www.urdupoint.com/kids/category/moral-stories-page10.html\n",
      "   โ Found 12 stories (Total: 108)\n",
      "\n",
      "๐ Page 11: https://www.urdupoint.com/kids/category/moral-stories-page11.html\n",
      "   โ Found 12 stories (Total: 120)\n",
      "\n",
      "๐ Page 12: https://www.urdupoint.com/kids/category/moral-stories-page12.html\n",
      "   โ Found 12 stories (Total: 132)\n",
      "\n",
      "๐ Page 13: https://www.urdupoint.com/kids/category/moral-stories-page13.html\n",
      "   โ Found 12 stories (Total: 144)\n",
      "\n",
      "๐ Page 14: https://www.urdupoint.com/kids/category/moral-stories-page14.html\n",
      "   โ Found 12 stories (Total: 156)\n",
      "\n",
      "๐ Page 15: https://www.urdupoint.com/kids/category/moral-stories-page15.html\n",
      "   โ Found 12 stories (Total: 168)\n",
      "\n",
      "๐ Page 16: https://www.urdupoint.com/kids/category/moral-stories-page16.html\n",
      "   โ Found 12 stories (Total: 180)\n",
      "\n",
      "๐ Page 17: https://www.urdupoint.com/kids/category/moral-stories-page17.html\n",
      "   โ Found 12 stories (Total: 192)\n",
      "\n",
      "๐ Page 18: https://www.urdupoint.com/kids/category/moral-stories-page18.html\n",
      "   โ Found 12 stories (Total: 204)\n",
      "\n",
      "๐ Page 19: https://www.urdupoint.com/kids/category/moral-stories-page19.html\n",
      "   โ Found 12 stories (Total: 216)\n",
      "\n",
      "๐ Page 20: https://www.urdupoint.com/kids/category/moral-stories-page20.html\n",
      "   โ Found 12 stories (Total: 228)\n",
      "\n",
      "======================================================================\n",
      "โ Total stories collected: 228\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[1/228] ูพูุฑุงุณุฑุงุฑ ุจูฺฺพุง\n",
      "   โ 18 paragraphs extracted\n",
      "   ๐ Preview: ุจุณ ูุฌฺพ ูุชุญ ฺฏฺฺพ ฺฉ ุงุณูนุงูพ ูพุฑ ุงูุชุงุฑ ฺฉุฑ ุขฺฏ ุจฺฺพ ฺฏุฆูฺบ ู ฺฉุณ ุณูุงุฑ ฺฉ ุชูุงุด ูฺบ ุงู...\n",
      "[2/228] ูุง ุนุฒู\n",
      "   โณ Timeout - Retry 1/3\n",
      "   โณ Timeout - Retry 2/3\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุง ูุงู ุญุณู ูฺบ ุงฺฉ ฺุงฺฉูนุฑ ูฺบุจฺูพู ูฺบ ูุฑ ูุงูุฏู ู ูุฌฺพ ุฎูุจ ูุงุฒูฺบ ุณ ูพุงู...\n",
      "[3/228] ฺฏฺพุฑ ูฺบ ูุฌุฑู\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฑุง ุชู ููุฒู ฺฏฺพุฑ ุจุณุช ฺฉ ุจุงูฺฉู ุขุฎุฑ ูฺบ ูุงูุน ุชฺพุงุงุณ ุณ ุชฺพูฺุง ุณุง ุขฺฏ ุฌูฺฏู ุดุฑูุน ...\n",
      "[4/228] ุฌูุช ฺฉุง ุฑุงุณุช\n",
      "   โ 27 paragraphs extracted\n",
      "   ๐ Preview: โูฺฏุชุง  ุงููุฑุ ุงุจุง ฺฉ ูพูุฑ ุฌุงุฆุฏุงุฏ ูพุฑ ูุจุถ ฺฉุฑูุง ฺุงุชุง !โ ุงุณูู ู ุงูพู ุจฺพุงุฆ ุณ...\n",
      "[5/228] ุจฺพูุชูฺบ ฺฉุง ูุงู\n",
      "   โ 19 paragraphs extracted\n",
      "   ๐ Preview: ุฏุงุฏ ุฌุงู ูพููฺฏ ูพุฑ ุจูนฺพ ููฺพุ ูู ุงูุฑ ฺฏฺ ฺฉู ุจฺพูุชูฺบ ฺฉ ุฎูู ูุงฺฉ ฺฉุงูุงฺบ ุณูุง ุฑ ุชฺพ...\n",
      "[6/228] ุงูุงูุฏุงุฑ ฺฉุง ุงูุนุงู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงุญูุฏ ุงูุชุงุฆ ุบุฑุจ ูุงูุฏู ฺฉุง ุจูนุง ุชฺพุงุฏู ูฺฏุง ฺฉุฑ ูพฺฺพุชุง ุงูุฑ ุงฺฺพ ููุจุฑูฺบ ุณ ูพุงุณ ูุชุง...\n",
      "[7/228] ุฑุญู ฺฉุง ุตู\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ูพุฑุงู ููุชูฺบ ฺฉ ุจุงุช ุ ฺฉุณ ููฺฉ ูพุฑ ุงฺฉ ุธุงูู ุจุงุฏุดุง ุญฺฉููุช ฺฉุฑุชุง ุชฺพุงุงุณ ฺฉ ุฑุนุงุง ุจฺพ...\n",
      "[8/228] ุฑู ฺฉ ูุงูพุฑูุง\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ฺฉ ุงุณุชุงุฏ ูฺบ ฺฉูุงุณ ูฺบ ฺฉูุฆ ุงู ุจุงุช ุณูุฌฺพุง ุฑ ุชฺพู ุงู ุจุงุช ...\n",
      "[9/228] ูุช\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ูุฑุชุจ ฺฉ ุจุงุช  ฺฉ ูุฑ ูุงูุฏ ุตุงุญุจ ู ูฺบ ุจุชุงุง ฺฉ ู ูพฺฉูฺฉ ูพุฑ ุฌุง ุฑ ฺบู ...\n",
      "[10/228] ฺฉุฑู ูุฑุจุงู ุชู ุงูู ุฒูู ูพุฑ\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ููุฌูุงู ฺฉ ูุงฺบ ุณุฎุช ุจูุงุฑ ุงูุฑ ุณูพุชุงู ูฺบ ุงูุชุงุฆ ูฺฏุฏุงุดุช ฺฉ ุดุนุจ ูฺบ ุฏุงุฎู ุชฺพ...\n",
      "[11/228] ูพุงูฺุง ุงูุฑ ุงุณูุงุฑูน ููู\n",
      "   โ 25 paragraphs extracted\n",
      "   ๐ Preview: ุงูุณูุงู ุนูฺฉู! ุณุฑ ุจฺพุงูู ู ฺฉูุงุณ ูฺบ ุฏุงุฎู ูุช  ฺฉุงุณุจ ุงุณูนูฺููนุณ ู ุงููฺ ุขูุงุฒ ู...\n",
      "[12/228] ููู ฺฉ ุฎูุงุด\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ูพุงุฑ ุจฺู! ูุฑุง ูุงู ููู ูฺบ ุฏฺฉฺพู ูฺบ ุชู ุจุช ูุนููู ูฺฏุชุง ูฺบ ูฺฉู ูุฑ ฺฉุงุฑูุง...\n",
      "[13/228] ุงูููู ุฏูุณุช\n",
      "   โ 29 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุงุฒ ูู ูุฑฺฉ ฺฉุฑ ฺฉ ูุงุฑุบ ูุง ุชู ุงูพู ุฏุงุฏุง ุงุจู ฺฉ ูพุงุณ ฺูุง ุขุงุงุณ ฺฉ ุฏุงุฏุง ุงุจู ุญุณุจู ...\n",
      "[14/228] ูุงู ฺฉุง ุณุงูุงู\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุจุงุฑ ูุฑ ูุงูุฏ ุตุงุญุจ ุงฺฉ ุดุงุฏ ูฺบ ุดุฑฺฉุช ฺฉ ูุฆ ูุงูุฑ ุณ ฺฉุฑุงฺ ุฑูุงู ูุฆฺบุงูพู...\n",
      "[15/228] ุฎุงู ูพูุฌุฑ\n",
      "   โณ Timeout - Retry 1/3\n",
      "   โ 43 paragraphs extracted\n",
      "   ๐ Preview: ุฑุญูุช ุณุฑ ุฌฺพฺฉุงุฆ ูุฆ ฺฉฺพฺุง ุชฺพุงุฒูู ูพุฑ ุฑฺฉฺพุง ุทูุท ฺฉุง ูพูุฌุฑ ุฎุงู ุชฺพุงุณุฑูุด ุทูุท ฺฉุง ุฎุง...\n",
      "[16/228] ุทุงูุชูุฑ ุจ ูููู\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview:  ุงฺฉ ุทุงูุชูุฑุ ูฺฉู ุงฺฉ ุจ ูููู ุงูุณุงู ฺฉ ฺฉุงู ุงุณ ฺฉุง ูุงู ุชู ฺฉฺฺพ ุงูุฑ ุชฺพุงุ ูฺฉู...\n",
      "[17/228] ูฺฉฺ ุงุฑ ฺฉุง ุจูนุง\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: โุชู ฺฉฺฺพ ูฺบ ฺฉุฑ ุณฺฉ ฺฏุงุชุฌฺพ ูฺฉฺุงฺบ ฺฉุงูน ฺฉุฑ ูุฑูุฎุช ฺฉุฑู ฺฉุง ฺฉุง ุชฺพุงูพฺ ฺฉุง ฺฉฺพู ุฏฺฉฺพู...\n",
      "[18/228] ุงฺุฏ ฺฉุง ูุนุฏ\n",
      "   โ 40 paragraphs extracted\n",
      "   ๐ Preview: ูุงฺบ ุจุงูพ ฺฉ ูุฑู ฺฉ ุจุนุฏ ุฏู ุจฺพุงุฆ ุงฺฉ ฺฏุงุคฺบ ูฺบ ุฑุช ุชฺพุจฺุง ุจฺพุงุฆ ุจุช ฺุณุชุ ุณูุฌฺพ ุฏุง...\n",
      "[19/228]  ุจฺฉุฑุง ฺฉุณ ฺฉุง ุ\n",
      "   โ 60 paragraphs extracted\n",
      "   ๐ Preview: ูพุฑ ุฌ ุงูพู ฺฏฺพุฑ ฺฉ ุตุญู ูฺบ ุงฺฉ ุจฺฉุฑ ฺฉู ุจ ูฺฉุฑ ุณ ูนูุช ูุฆ ุฏฺฉฺพ ฺฉุฑ ฺููฺฉ:โุงุฑ ...\n",
      "[20/228] ูฺฉ ฺฉุง ุตู\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: ุณฺฉูู ฺฉ ฺฏฺพููน ุจุฌุ ุณุงุฑ ุจฺ ุฏูฺ ุงูุฑ ฺฏูน ุณ ุจุงุฑ ูฺฉู ฺฏุฆุญุงูุฏ ุงูุฑ ุงุญูุฏ ุฏูฺ ฺฉุฑ ุจุณ...\n",
      "[21/228] ูฺฏู\n",
      "   โ 16 paragraphs extracted\n",
      "   ๐ Preview: โุงุฑุ ฺฉุง ุชู ุฑ ููุช ูพฺฺพุช ุฑุช ู! ฺูู ุงููนฺพูุ ฺฏุงฺ ุขุฆ ูุฆ  ูุฑูุช ฺฉ ูุฆโ ูููน...\n",
      "[22/228] ูพุฑ ฺฉ ููุชุง\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูพุฑุงู ุฒูุงู ฺฉ ุจุงุช  ฺฉ ูพุฑุณุชุงู ูฺบ ุณุงุช ูพุฑุงฺบ ุฑุช ุชฺพฺบู ุขูพุณ ูฺบ ฺฏุฑ ุณูุงฺบ ...\n",
      "[23/228] ุนูู ฺฉ ุงูุชุง ุฌุงูุช \n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏู ุญูู ุงูุฑ ุณูู ุฏูููฺบ ุงูพู ุงุจุง ฺฉ ุณุงุชฺพ ุณุฑ ฺฉู ฺฏุฆุญูู ุจฺุง ุชฺพุง ุงูุฑ ุณูู ฺฺพู...\n",
      "[24/228] ุณูพุฑ ุฑู\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุง ุขูพ ู ฺฉุจฺพ ุณูพุฑ ุฑู ุฏฺฉฺพุง ุ ุขูพ ู ุชู ุจุช ุณุงุฑ ุณูพุฑ ุฑู ฺฉ ูุงู ุณูฺ ูุฆ ูฺบ ...\n",
      "[25/228] ุชููุฑ ฺฉ ุฒูุฏฺฏ\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุฑู ุงูพู ฺฏุงุคฺบ ฺฉ ุจฺ ุฌุงฺฏุฑุฏุงุฑ ฺูุฏุฑ ุณููุงู ฺฉ ุงฺบ ููุงุฒูุช ฺฉุฑุชุง ุชฺพุงู ฺูุฏุฑ ุตุงุญ...\n",
      "[26/228] ฺฉูุง\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ูพุงฺฉุณุชุงู ูฺบ ุจุช ุณ ูพุฑูุฏ ูพุงุฆ ุฌุงุช ฺบฺฉุจูุชุฑุ ฺฺุงุ ุทูุทุงุ ููุงุ ุชุชุฑุ ุจูนุฑุ ฺู ...\n",
      "[27/228] ฺฺพููน ุณ ุฎุจุฑ\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ุงุฎุจุงุฑ ฺฉ ุงฺฉ ฺฺพููน ุณ ุฎุจุฑ ูพฺฺพ ฺฉุฑ ุนุงุฆุด ู ูุญุณูุณ ฺฉุงุ  ุฏู ูุงุฆููฺบ ฺฉ ุฎุจุฑ ู ุฌุงู...\n",
      "[28/228] ุงูุนุงู\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ูุนูุงู ฺฉู ุดุฏุช ุณ ุงูุชุธุงุฑ ุชฺพุง ฺฉ ุงุณ ฺฉ ุงุจู ุฌุงู ุงุณ ฺฉ ูุฆ ูพุงฺฉุณุชุงู ฺฉุง ุงฺฉ ุจฺุง ุณุง ุฌฺพูฺ...\n",
      "[29/228] ูฺฉ\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุดุฑ ฺฉ ุงฺฉ ูุดูุฑ ุตูุนุช ฺฉุงุฑ ูุงุฑูู ุจฺฏ ฺฉ ุจูน ฺฉ ุดุงุฏ ุชฺพุงฺฉ ูุช ูพู  ุดุงุฏ ฺฉุง...\n",
      "[30/228] ุฏูุณุช ู ุฌู ูุตุจุช ูฺบ ฺฉุงู ุขุฆ\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุฌูฺฏู ูฺบ ุงฺฉ ูุบุฑูุฑ ุงูููน ุฑุชุง ุชฺพุงุงุณ ุงูพู ููุจ ูุฏ ุงูุฑ ฺุงู ฺฺพุงู ูพุฑ ุจฺุง ูุงุฒ ุชฺพ...\n",
      "[31/228] ูพฺฉ ุฏูุณุช\n",
      "   โ๏ธ No story content extracted\n",
      "[32/228] ฺฺพููน ุณ ูฺฉ\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ูุถู ุงููฐ ุขุฌ ุจุช ุฎูุด ุชฺพุงู ุขุฌ ุงูพู ุณุจ ุณ ุจฺ ุจูน ฺฉู ุดุฑ ฺฉ ูุงููุฑ ฺฉุงูุฌ ูฺบ ุฏุงุฎู...\n",
      "[33/228] ุฏุงูุช ฺฉุง ุตู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฌุฏ ุงฺฉ ฺุงุฆ ฺฉ ูฺฉูนุฑ ูฺบ ุณู ุฑููพ ูุงูุงุฑ ูพุงุชุง ุชฺพุงุงุณ ฺฉ ุจู ุงูุฑ ุฏู ุจฺ ุฑุถูุงู ...\n",
      "[34/228] ูพุฑูุฏูฺบ ฺฉุง ูพุงฺฉุณุชุงู\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ู ุงฺฉ ุฎูุจุตูุฑุช ุจุงุบ ุชฺพุงุฌุณ ูฺบ ุชุงุญุฏู ูฺฏุง ุฑุงู  ุฑุงู ุชฺพุงฺฉ ุฌุงูุจ ูุฎุชูู ูพฺพู...\n",
      "[35/228] ุจุฑฺฏุฏ ฺฉุง ุฏุฑุฎุช\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุฌูฺฏู ูฺบ ุงฺฉ ุฏู ุณูุฑ ุณูุฑ ูฺู ูฺ ฺฏุฆุฑ ุทุฑู ูพุฑูุฏ ุดูุฑ ูฺุง ุฑ ุชฺพุ ุฎุฑฺฏูุด ุฏูฺุช...\n",
      "[36/228] ุนูู ุฎุฒุงู\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุฏููุช ุฎุฑฺ ฺฉุฑู ุณ ฺฏฺพูนุช ฺฉุง ฺฉูุฆ ุงุณ ุฏููุช ุจฺพ ุ ุฌู ุฎุฑฺ ฺฉุฑู ุณ ุจฺฺพุช ! ุง...\n",
      "[37/228] ูุงูฺ ฺฉุง ุงูุฌุงู\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ฺฏุงุคฺบ ูฺบ ุฏู ุจฺพุงุฆ ุฑุช ุชฺพุจฺุง ุจฺพุงุฆ ุงูุฑ ุชฺพุง ุงูุฑ ฺฺพููนุง ุจฺพุงุฆ ุบุฑุจุ ูฺฉู ุฑ ...\n",
      "[38/228] ฺฉฺู ูพุช\n",
      "   โ 15 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุตุจุญ ฺู ฺฉุง ุจุงุฏุดุง ุชฺพฺฉู ฺฉ ูุฌ ุณ ุบููุฏฺฏ ูุญุณูุณ ฺฉุฑ ุฑุง ุชฺพุงุงุณ ุจุช ุณุงุฑุง ฺฉุงู ฺฉุฑ...\n",
      "[39/228] ุงุตู ูุญุณู\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุนู ุงฺฉ ูุงุฆู ุทุงูุจ ุนูู ุชฺพุงููฺบ ุฌูุงุนุช ุงุณ ู ุงฺฺพ ููุจุฑูฺบ ุณ ูพุงุณ ฺฉุฑ ูุ ูฺฉู ุฏุณูฺบ ...\n",
      "[40/228] ุฏูุณุฑุง ุฑุงุณุช\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: โุณูนฺพ ุตุงุญุจ! ูุฌฺพ ุตุฑู ุฏู ุณู ุฑููพ ุฏ ุฏุฌุขูพ ูุฑ ุงูุฌุฑุช ูฺบ ุณ ฺฉุงูน ูุฌ ฺฏุงุฎุฏุงุฑุง...\n",
      "[41/228] ุขูุณูุคฺบ ฺฉ ุทุงูุช\n",
      "   โ 40 paragraphs extracted\n",
      "   ๐ Preview: ุชููฺบ ุงฺฉ ูพุงุฑฺฉ ูฺบ ุจูนฺพ ุชฺพุจูนู ูฺบ ุณ ูฺฉูู ูุงู ุฑูู ุงู ฺฉ ุณุงูู ุชฺพุชู ุณู ุณ...\n",
      "[42/228] ุงุญูุฏ ฺฉ ฺฉุงู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูู ูุฑ ฺฉ ฺฉูุงุฑ ุงฺฉ ุญุณู ฺฏุงุคฺบ ุขุจุงุฏ ุชฺพุงูุฑ ฺฉุง ุดูุงู ูพุงูุ ุณุจุฒ ฺฉฺพุชูฺบ ฺฉ ููุงูน ...\n",
      "[43/228] ุฌูฺฏู ูฺบ ูฺฏุงู\n",
      "   โ 38 paragraphs extracted\n",
      "   ๐ Preview: ุณูุฑุฌ ุงุจฺพ ูพูุฑ ุทุฑุญ ุทููุน ุจฺพ ูฺบ ูุง ุชฺพุง ฺฉ ุฌูฺฏู ูฺบ ุงฺฉ ูฺฏุงู ูฺ ฺฏุงูู ูพุงู...\n",
      "[44/228] ุฌุงุฏูุฆ ูุฑุบ\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุบุฑุจ ุขุฏู ฺฉ ูพุงุณ ุงฺฉ ฺุงูุงฺฉ ุงูุฑ ุฌุงุฏูุฆ ูุฑุบ ุชฺพุงฺฉ ุฏู ุงุณ ูุฑุบ ฺฉู ุณฺฺฉ ูพุฑ ุงฺฉ ...\n",
      "[45/228] ุนูุฑ ฺฉุง ุณุจู\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: โุนูุฑโคโคโคโคโคุนูุฑโคโคโคโคุงููนฺพู ุจูนุง!โ ุงู ู ุนูุฑ ฺฉู ุฌฺฏุงู ฺฉ ฺฉูุดุด ฺฉ\n",
      "[46/228] ูพูุฑุงุณุฑุงุฑ ูุงุฏ\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุงุณุฑุ ุนุงุฏูุ ูุฏ ุงูุฑ ุนูุฑ ฺุงุฑ ุฏูุณุช ุชฺพฺุงุฑูฺบ ุฏูุณุชูฺบ ฺฉู ูู ุฌูุฆ ฺฉุง ุจุช ุดูู ุชฺพุงุงฺฉ...\n",
      "[47/228] ุจุงุฏุฑ ุฑู ฺฉุง ุจฺ\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฌูฺฏู ูฺบ ุณุจ ุฌุงููุฑ ูุณ ุฎูุด ุงุชูุงู ุณ ุฑุช ุชฺพุงฺุงูฺฉ ุฌูฺฏู ุณ ุฌุงููุฑ ุบุงุฆุจ ูู ...\n",
      "[48/228] ฺฏฺู ุงูุฑ ุขู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ฺฏฺู ุงฺฉ ูฺฉ ุจฺ ุชฺพุงุ ุฌู ูพฺฺพุงุฆ ูฺบ ุจฺพ ุงูู ุขุชุง ุชฺพุงูพฺพููฺบ ูฺบ ุงุณ ุขู ุจุช ูพุณูุฏ ...\n",
      "[49/228] ูุญูุช ูฺฺฉุง\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุงุดุนุฑ ุงฺฉ ูฺฉฺุงุฑุง ุชฺพุง ุฌู ุงูพู ุจูุงุฑ ูุงฺบ ฺฉ ุฏฺฉฺพ ุจฺพุงู ฺฉุฑุชุง ุงูุฑ ุตุจุญ ุณ ุดุงู ุชฺฉ ูฺฉฺุง...\n",
      "[50/228] ุจุฑููุช\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ูฺบ ุงฺฉ ุณุฑฺฉุงุฑ ุงุณูพุชุงู ูฺบ ฺุงฺฉูนุฑ ูฺบุนุงู ุทูุฑ ูพุฑ ูุฑ ฺููน ุฑุงุช ฺฏุฆ ุชฺฉ ุฌุงุฑ ุฑุช ...\n",
      "\n",
      "   ๐ Restarting browser (processed 50 stories)...\n",
      "\n",
      "[51/228] ุจุงุบ ฺฉ ุณุฑ\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุนุจุฏุงูุงุฏ ุงูุฑ ูุงูุน ุจู ุจฺพุงุฆ ุชฺพุฏูููฺบ ุฐู ูู ฺฉ ุณุงุชฺพ ูุฆ ูุฆ ุดุฑุงุฑุชูฺบ ฺฉ ูู...\n",
      "[52/228] ุงูุฏ ฺฉุง ฺุงูุฏ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: โููุฑฺ ฺฉฺพูุฏ ูุง ฺฏุง ! ุณุฑ ุฌููุฌ ุฌูุงู ู ุข ฺฉุฑ ุจุชุงุงุ ุชู ฺฉูุงูฺุฑ ูุงุฆู ุฌู ุขุณูุงู ูพ...\n",
      "[53/228] ูุดุงุฏ ุงูุฑ ฺฉุงู\n",
      "   โ 27 paragraphs extracted\n",
      "   ๐ Preview: โูุจุงุฑฺฉ ู ุญูู! ุงุณ ูุง ูพฺพุฑ ุชูุงุฑ ฺฉุงู ุดุงุฆุน ูุฆ โ ุญูู ฺฉ ฺฏฺพุฑ ูฺบ ุฏุงุฎู ูุช...\n",
      "[54/228] ุฌุงุฏูุฆ ฺฺพฺ\n",
      "   โ 27 paragraphs extracted\n",
      "   ๐ Preview: ุดุงุจู ุงฺฉ ุจุณุช ฺฉุง ุบุฑุจ ฺฏฺุฑุง ุชฺพุงุงุณ ฺฉ ูพุงุณ ุงฺฉ ููุจ ุฎูุจุตูุฑุช ฺฺพฺ ุชฺพ ุฌุณ ุณ ู ุจฺฉุฑ...\n",
      "[55/228] ุงุญุณุงุณ\n",
      "   โ 18 paragraphs extracted\n",
      "   ๐ Preview: ุฑูุถุงู ุงููุจุงุฑฺฉ ฺฉุง ุขุฎุฑ ุนุดุฑ ุชฺพุงุนุฏ ฺฉ ุชุงุฑูฺบ ู ุจฺพ ุฒูุฑ ูพฺฉฺ ูุง ุชฺพุงุจฺ ุชู ุนุฏ ...\n",
      "[56/228] ูุญูุช ฺฉุง ุตู\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุดูุดุง ฺฉ ูุงูุฏ ุงฺฉ ุณุฑฺฉุณ ูฺบ ฺฉุงู ฺฉุฑุช ุชฺพุ ฺฏุงุคฺบ ฺฏุงุคฺบุ ุจุณุช ุจุณุช ฺฏฺพููุช ุงูุฑ ุฌู ุขูุฏ...\n",
      "[57/228] ฺูฺฉ ูุงู ฺฉฺพููู\n",
      "   โ 20 paragraphs extracted\n",
      "   ๐ Preview: โุจูน! ุฐุฑุง ูพุงู ุชู ฺุงูู ุงุณ ูฺบโ ุงูู ุฑฺฉฺพุง ฺฉูุงุฑ ู ุงูพู ุจูน ูุฑู ุณ ฺฉุง ุฌู ุจ...\n",
      "[58/228] ูพูุฏูฺบ ฺฉ ุงูุช\n",
      "   โ 2 paragraphs extracted\n",
      "   ๐ Preview: ุขุฌ ุงุณฺฉูู ูฺบ ูพูุฏูฺบ ฺฉ ุงูุช ู ุงูุงุฏุช ุณ ูุชุนูู ุงฺฉ ุณููุงุฑ ููุนูุฏ ฺฉุง ฺฏุง ุชฺพุงุ ุฌุณ ...\n",
      "[59/228] ฺฉูููพู ฺฉ ฺฉุงู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูุญู ูฺบ ุจุงุฑุด ู ุฑ ุชฺพ ุงูุฑ ฺูุฏ ุจฺ ุจุงุฑ ฺฉฺพู ุฑ ุชฺพุงูฺพูฺบ ู ุฏฺฉฺพุง ฺฉ ูุง ฺฉ ...\n",
      "[60/228] ฺฉู ุธุฑู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ููุงุช ุณ ูพู ูุงูุฏ ุตุงุญุจ ู ุงฺฉ ฺูุชุง ูุง ุงุณฺฉูู ูุฑ ุญูุงู ฺฉุง ุงูุฑ ุงุจ ุญุงู  ุชฺพุง ฺฉ...\n",
      "[61/228] ูุงูุฑูุงู ฺฉ ุณุฒุง\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ู ุงฺฉ ุจุช ุดุฑุงุฑุช ฺูุง ุชฺพุงุ ุฌู ุงูพู ูุงฺบ ุจุงูพ ฺฉ ุณุงุชฺพ ุฑุชุง ุชฺพุงุงุณ ฺฏฺพุฑ ูฺบ ุงฺฉ ุจู ...\n",
      "[62/228] ูุงุถ ฺฉุง ุงูุตุงู\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ฺฏุงุคฺบ ูฺบ ุงุฑูุงู ูุงู ุขุฏู ุฑุชุง ุชฺพุง ุงูุณ ฺฉ ูพุงุณ ฺฉูุฆ ููฺฉุฑ ูฺบ ุชฺพุงฺฉ ุฏู ุงูุณ ...\n",
      "[63/228] ูพูุฑ ุงุณุฑุงุฑ ุบุงุฑ\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ููุตูุฑ ฺฏุงุคฺบ ูฺบ ุงูพู ูุงฺบ ฺฉ ุณุงุชฺพ ุฑุง ฺฉุฑุชุง ุชฺพุงุงุณ ฺฉ ูุงูุฏ ฺฉุง ุงูุชูุงู ู ฺฺฉุง ุชฺพุง ุงูุฑ...\n",
      "[64/228] ฺฉุงููนูฺบ ฺฉ ุฏุฑูุงู\n",
      "   โ 19 paragraphs extracted\n",
      "   ๐ Preview: ุณุงุชฺพู!  ูุต  ุจููุง ฺฉุง ุฌุณ ุงูฺฏุฑุฒ ูฺบ Baya Weaver ฺฉุช ฺบฺฉุงู ุณูุงู ุณ ูพ...\n",
      "[65/228] ุงููฺฉฺพ ุฏูุง\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ููู ุฏุฑุง ฺฉ ฺฉูุงุฑ ุงฺฉ ูุตุจ ูฺบ ุงูพู ุจูฺฺพ ูุงู ฺฉ ุณุงุชฺพ ุฑุชุง ุชฺพุงุตุจุญ ู ุงุณฺฉูู ุฌุง...\n",
      "[66/228] ุดุงูุชู ุฌู\n",
      "   โ 23 paragraphs extracted\n",
      "   ๐ Preview: ุงุญูุฏ ุงุณฺฉูู ุณ ูุงูพุณ ุข ุฑุง ุชฺพุงุฑุงุณุช ูฺบ ุงุณ ฺูฺฉุชุง ูุง ฺุฑุงุบ ูุธุฑ ุขุงุฏููพุฑ ฺฉ ุชุฒ ุฏ...\n",
      "[67/228] ูฺบ ูพุงฺฉุณุชุงู ููุฌ ูฺบ\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ูุฌฺพ ุขูพ ุงฺฺพ ุทุฑุญ ุฌุงูุช ฺบ ฺฉ ูุฑุง ฺฉุงู ูพุงฺฉุณุชุงู ฺฉ ุญูุงุธุช ฺฉุฑูุง ูพุงฺฉุณุชุงู ฺฉู ุงุณ ฺฉ...\n",
      "[68/228] ฺฏุฑู ฺฉุง ุณุจุจ\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: โุจฺพุงุฆ! ุขุฌ ุชู ฺฏุฑู ุณ ุจูุฑุง ุญุงู ู ุฑุง โ ุงุญูุฏ ุจุฌู ุขุช  ูพูฺฉฺพ ฺฉ ุณุงูู ุจูนฺพุช...\n",
      "[69/228] ูฺฉ ฺฉุง ุจุฏู\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: ุฌูฺฏู ูฺบ ูุฏ ฺฉ ฺฉูุงุฑ ุงฺฉ ุฏุฑุฎุช ฺฉ ุชู ูฺบ ููฺพ ฺูููน ู ุงูพูุง ฺฏฺพุฑ ุจูุงุง ูุง ุชฺพุง ...\n",
      "[70/228] ูฺฉ ฺฉ ุชูุงุด\n",
      "   โ 20 paragraphs extracted\n",
      "   ๐ Preview: ู ุงฺฉ ููฺพ ูู ูพุงุฑ ุณ ูุงุฒฺฉ ุณ ฺฺุง ุชฺพู ฺูฺบ ฺูฺบ ฺฉุฑุช ฺฏฺพุฑูฺบ ฺฉ ููฺุฑ ุงูุฑ ุฏุฑุฎ...\n",
      "[71/228] ฺฉูู ุญููุงุฆ\n",
      "   โ 31 paragraphs extracted\n",
      "   ๐ Preview: ุฌุณ  ูุบุฑุจ ฺฉ ุงุฐุงู ูุฆฺฉู ูุงู ฺฉ ุฌู ุจฺ ุงุณฺฉูู ุฌุงู ฺฉ ูุฆ ุชุงุฑ ูู ูฺฏ\n",
      "[72/228] ุณฺฉููุณุชุงู\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุจุช ูพู ฺฉ ุจุงุช  ููฺฉ ุณฺฉููุณุชุงู ูพุฑ ุงฺฉ ุจุงุฏุดุง ุญฺฉููุช ฺฉุฑุชุง ุชฺพุงู ุจุช ุฑุญู ุฏู ุงูุฑ ู...\n",
      "[73/228] ฺฺพููน ุณ ูฺฉฺ ุจฺ ุณ ุจุงุช\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏู ูููู ุงูพู ฺฉูุฑ ูฺบ ุจูนฺพ ุชฺพฺฉฺพฺฺฉ ฺฉ ฺฉูู ูฺบ ุงฺฉ ููฺพ ุณ ูฺฉฺ ุฌุงูุง ุจูุง...\n",
      "[74/228] ุณุงุช ุฑูฺฏ ฺฉุง ูพฺพูู\n",
      "   โ 31 paragraphs extracted\n",
      "   ๐ Preview: ูพุฑุงู ุฒูุงู ฺฉ ุจุงุช ูุดุฑู ูพุงฺูฺบ ฺฉ ุฏุฑูุงฺบ ุงฺฉ ุจฺ ุงุณูุงู ุฑุงุณุช ุชฺพุ ุฌุณ ูพุฑ ุจุงุฏ...\n",
      "[75/228] ุณู ุจูุฑ ฺฏูุฏู\n",
      "   โ 27 paragraphs extracted\n",
      "   ๐ Preview:  ูุงุฑุซ ูฺฏุฑ ฺฏุงุคฺบ ฺฉ ฺฉุงู ุงุณ ฺฏุงุคฺบ ฺฉู ูุงุฑุซ ูุงู ุงฺฉ ุขุฏู ู ุขุจุงุฏ ฺฉุง ุชฺพุง ฺฏุง...\n",
      "[76/228] ุงูุชุญุงู\n",
      "   โ 27 paragraphs extracted\n",
      "   ๐ Preview: โฺฉุง ูุง ุซุงูุจ ุจูนุง! ฺฉุณุง ุฑุง ุงููนุฑููุโ ุซุงูุจ ฺฉู ุขุชุง ุฏฺฉฺพ ฺฉุฑ ุจุงุจุง ู ุขูุงุฒ ูฺฏุงุฆ\n",
      "[77/228] ูุณู\n",
      "   โ 53 paragraphs extracted\n",
      "   ๐ Preview: ุดุงุฏุงุจ ฺฉุงูููุ ุงูพู ูุงู ฺฉ ุญุฏ ุชฺฉ ุดุงุฏุงุจ ุชฺพูุฆ ุขุจุงุฏ ูู ูุงู ุงุณ ฺฉุงููู ูฺบ ุฑู ...\n",
      "[78/228] ุฏุงุฑ ฺู ฺฉ ุขูุณู\n",
      "   โ 18 paragraphs extracted\n",
      "   ๐ Preview: ฺฏุฑู ูุตุงูุญ ุฒูุฑ ุดูุฑ ุณ ุฏุงุฑ ฺู ฺฉ ูุฎุงููุช ฺฉุฑ ุฑ ุชฺพฺฉุงู ูุฑฺ ุงูุฑ ูููฺฏ ุฒุงุฏ ุบุต ...\n",
      "[79/228] ฺูููนุงฺบ ุงฺฉ ุฏูุณุฑ ฺฉู ฺฉุณ ูพฺุงูุช ฺบ\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ฺูููนูฺบ ฺฉ ุฌุณู ุณ ุขู ูุงู ูฺฉ ุงฺฉ ุฌุณ ูุช ุงุณ ุณ ู ุงฺฉ ุฏูุณุฑ ฺฉู ูพฺุงูุช ...\n",
      "[80/228] ุจ ุฒุจุงู ูุญุณู\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุนุงูุฑ ุจฺูพู  ุณ ุงูพู ูุงูุง ูุงู ฺฉ ุณุงุชฺพ  ุฑุชุง ุชฺพุงู ุงุณฺฉูู ุงูุฑ ูุฏุฑุณ ุณ ูุงุฑุบ ู...\n",
      "[81/228] ุจููุชุง ุฌูฺฏู\n",
      "   โ 24 paragraphs extracted\n",
      "   ๐ Preview: ุดุฒุฑุงู ูุงู ฺฺพููน ุณ ูุตุจ ูฺบ ุนู ุงูพู ุฏุงุฏุง ฺฉ ุณุงุชฺพ ูุณ ุฎูุด ุฑุช ุชฺพุงู ฺฉุง ฺฏฺพุฑ...\n",
      "[82/228] ุชุฑุจุช\n",
      "   โ 19 paragraphs extracted\n",
      "   ๐ Preview: ุณูู ฺฏฺพุฑ ูฺบ ุฏุงุฎู ูุง ุชู ุงุณ ฺฉุง ูู ุงูุชุฑุง ูุง ุชฺพุงู ุณุฏฺพุง ุงูพู ฺฉูุฑ ูฺบ ฺฏุงุงุณ ฺฉ...\n",
      "[83/228] ฺฉุณ ุฑุ\n",
      "   โ 23 paragraphs extracted\n",
      "   ๐ Preview: ุงู ุณุงุชูฺบ ุฌูุงุนุช ูฺบ ูพฺฺพุช ุชฺพู ุงฺฉ ูุงุฆู ุจฺ ุชฺพู ุงูพูุง ุณุจ ฺฉุงู ููุช ูพุฑ ฺฉุฑุช ...\n",
      "[84/228] ูฺฉุงุฑ ุฌุงุฏูฺฏุฑู\n",
      "   โ 16 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ฺฏุงุคฺบ ูฺบ ุงฺฉ ฺฉุณุงู ุงูพู ุจู ฺฉ ุณุงุชฺพ ุฑุชุง ุชฺพุงฺฉุณุงู ฺฉ ุจู ู ฺฉุณุงู ุณ ฺฉุง:โุขุฌ...\n",
      "[85/228] ุฎุฑฺฏูุด ูพูพู ฺฉุง ูุนุฏ\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ฺฺพููนุง ุณุง ูุฑู ู ูุงุฒฺฉ ุฎุฑฺฏูุด ูพูพู ุขุฌ ุจุช ุบุต ูฺบ ุชฺพุงุงุณ ฺฉุง ุฏู ฺฉุฑ ุฑุง ุชฺพุง ฺฉ ุณุจ ฺุฒฺบ...\n",
      "[86/228] ฺุงูุงฺฉ ูููฺ\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุฌูฺฏู ูฺบ ุงฺฉ ูููฺ ุฑุช ุชฺพูููฺ ุงูุชุงุฆ ฺุงูุงฺฉ ุชฺพู ุดฺฉุงุฑ ฺฉ ุชูุงุด ูฺบ ุฌูฺฏู...\n",
      "[87/228] ุฎูุฏ ุบุฑุถ\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุณุฑุณุจุฒ ู ุดุงุฏุงุจ ูุงุฏ ูฺบ ูุณุน ู ุนุฑุถ ุจุงุบ ูุงูุน ุชฺพุงุจุงุบ ฺฉ ูุณุท ูฺบ ุงฺฉ ุจุช ูพุฑุงูุง...\n",
      "[88/228] ฺูฺฏุงุฏฺ ฺฉ ฺุงูุงฺฉ\n",
      "   โ 16 paragraphs extracted\n",
      "   ๐ Preview: ุฌูฺฏู ูฺบ ุงฺฉ ุดุฑ ุงูพู ุบุงุฑ ฺฉ ุจุงุฑ ุดฺฉุงุฑ ฺฉุฆ ูุฆ ฺฏูุดุช ฺฉุง ุงฺฉ ูนฺฉฺุง ูุฆ ฺฉฺพุงู  ูุง...\n",
      "[89/228] ฺูุฑูฺบ ฺฉุง ุงูุฌุงู\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุฑุงุฌ ุฑุงู ุงฺฉ ูุฏู ูุญู ููุง ูฺฉุงู ูฺบ ุฑุช ุชฺพุงููฺบ ู ุงูพู ุญูุงุธุช ฺฉ ูุฆ ุชู ุณฺฉู...\n",
      "[90/228] ููุงู ููุงุฒ\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุจุฑุณุงุช ฺฉ ุฏู ุชฺพุ ุฑ ุทุฑู ุจุงุฑุด ุณ ููุณู ุฎูุดฺฏูุงุฑ ุงูุฑ ุณฺฺฉฺบ ูพุงู ุณ ุฌู ุชฺพู ุชฺพฺบุจุงุฑุด ...\n",
      "[91/228] ุจุฏ ฺฏูุงู\n",
      "   โ 51 paragraphs extracted\n",
      "   ๐ Preview: ุดุฑุงุฒ ุงูพู ุฏูููฺบ ูพููุคฺบ ูพุฑ ุงุชฺพ ุฑฺฉฺพ ุงูุฏฺพุฑ ุงูุฏฺพุฑ ุจฺฉฺพุฑ ูุฆ ฺุฒูฺบ ูพุฑ ูุธุฑฺบ ุฏูฺุง ...\n",
      "[92/228] ุดฺฉุฑ ฺฏุฒุงุฑ\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ุนูุงุจ ุงฺฉ ุบุฑุจ ุจฺ ุชฺพ ุฌู ุงูพู ูุงู ฺฉ ุณุงุชฺพ ุฑุช ุชฺพุฑูุถุงู ุงููุจุงุฑฺฉ ฺฉ ุขูุฏ ุขูุฏ ุช...\n",
      "[93/228] ุฒูุจ ู ฺฉฺพุฑ ูพฺฉุงุฆ\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุฒูุจ ุณุงุชูฺบ ุฌูุงุนุช ฺฉ ุทุงูุจ ุชฺพุ ุงุณ ฺฏุฑููฺบ ฺฉ ฺฺพูนุงฺบ ู ฺฺฉ ุชฺพฺบุฒูุจ ฺฉ ุงู ุงุณ ...\n",
      "[94/228] ุฏุงูุง ุจุงุฏุดุง\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ุจุช ุนุฑุต ูพู ุงฺฉ ุฏูุฑ ุงูุชุงุฏ ุฑุงุณุช ูฺบ  ุฑูุงุช ุชฺพ ฺฉ ุฑ ุณุงู ูุง ุจุงุฏุดุง ููุชุฎุจ ฺฉ...\n",
      "[95/228] ุจุฑฺฏุฏ ุจูฺูฺฏ\n",
      "   โ 21 paragraphs extracted\n",
      "   ๐ Preview: ฺฏุฑููฺบ ฺฉ ุงฺฉ ุฏููพุฑุ ุญุจุณ ฺฉุง ุณูุงฺบ ุชฺพุงูพูุฏ ุฌฺพูุณุง ุฏู ูุงู ุฏฺพููพ ู ุฑ ฺุฒ ฺฉู ุงูพู...\n",
      "[96/228] ูฺฉ ุฏู ุจุงุฏุดุง\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ููฺฉ ูฺบ ุงุณุง ูุงููู ุชฺพุง ฺฉ ุฌุณ ฺฉู ุจุงุฏุดุง ุจูุงุง ุฌุงุชุงุ ุชู ุณุงู ฺฉ ุจุนุฏ ุงุณ ฺฏฺพู ุฌ...\n",
      "[97/228] ุจุฏูุงู ุจูุฑุง\n",
      "   โ 16 paragraphs extracted\n",
      "   ๐ Preview: ุญุงุฑุซ ุงูุฑ ูุงุฑุณ ุงูพู ูฺฺฉูพู ฺฉ ุงุจุชุฏุงุฆ ุฏูุฑ ูฺบ ุชฺพุงฺฉ  ุงุณฺฉูู ุงูุฑ ุงฺฉ  ุฌูุงุนุช ฺฉ...\n",
      "[98/228] ฺุงฺุง ุตุฏุฑ ุฏู\n",
      "   โ 21 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฑ ุฒูุฏฺฏ ูฺบ ุจุนุถ ุงุณ ูุงูุนุงุช ุจฺพ ุฑูููุง ูุช ฺบุ ุฌูฺบ ู ุจฺพูุงูุง ุจฺพ ฺุงฺบ ุชู...\n",
      "[99/228] ูุฑุง ูุดู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ูฺบ ูพู ฺฏุฑููพ ฺฉุง ุทุงูุจ ุนูู ูฺบ ูุฑ ุจฺ ุจู ุนูุด ูุงุทู ฺฉูุงุณ ูนู ูุฑ ุฏูุณุฑ ุจู ุงุฑุจ...\n",
      "[100/228] ุจุงุฑุด ฺฉุง ููฺพุง ูุทุฑ\n",
      "   โ 17 paragraphs extracted\n",
      "   ๐ Preview: ุจุญุฑ ูุชูุณุท ุณ ุจุฎุงุฑุงุช ฺฉุง ุงฺฉ ุจฺฏูู ูุถุง ูฺบ ุจููุฏ ูุง ุงูุฑ ูุง ฺฉ ุฏูุด ูพุฑ ุงูุฑู ฺฉ ...\n",
      "\n",
      "   ๐ Restarting browser (processed 100 stories)...\n",
      "\n",
      "[101/228] ุฎูุงุจ ฺฉ ุชุนุจุฑ\n",
      "   โ 24 paragraphs extracted\n",
      "   ๐ Preview:  ฺฉุงู  ุงฺฉ ูพุฑ ฺฉูฺบ ูฺบุ ู ฺฉู ูุงู ฺฉ ูพุฑ ูฺบุ ุฌุณ ุฌู ุงููนฺพุง ฺฉุฑ ู ุฌุงุช...\n",
      "[102/228] ูุฏ ฺฉ ุงุฐุช\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุจููน ุงฺฉ ุจุช  ูพุงุฑุง ุจฺ ุชฺพุงู ูฺบ ุชู ุงูพู ูุงูุฏู ุงูุฑ ุงุณุงุชุฐ ฺฉุง ุจุช ูุฑูุงูุจุฑุฏุงุฑ...\n",
      "[103/228] ุณฺ ุฏูุณุช\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: โุงุฑ ุงุญูุฏ! ุชู ุงุณฺฉูู ฺฉูฺบ ูฺบ ุข ุฑ ุชฺพุ ุงูุฑ  ุชู ููฺฏฺุง ฺฉุฑ ฺฉูฺบ ฺู ุฑ ูุโ ุนุจุฏ...\n",
      "[104/228] ุดฺฉุฑ ูฺู\n",
      "   โ 51 paragraphs extracted\n",
      "   ๐ Preview: ู ุนูุฑุช ุชุฒ ุณ ฺูุช ูุฆ ุงฺฉ ุทุฑู ุฌุง ุฑ ุชฺพุงุณ ฺฉ ฺฉูุฏฺพ ูพุฑ ุงฺฉ ุจฺุง ุณุง ูพุฑุณ ููนฺฉุง ...\n",
      "[105/228] ฺฏููุงู ุงูุฑ ููู ูพุฑ\n",
      "   โณ Timeout - Retry 1/3\n",
      "   โ 15 paragraphs extracted\n",
      "   ๐ Preview: ฺฏููุงู ฺฉุง ุชุนูู ุงฺฉ ุบุฑุจ ฺฏฺพุฑุงู ุณ ุชฺพุงู ุจุช  ุฑุญูุฏู ุงูุฑ ุจุงุฏุฑ ูฺฺฉุง ุชฺพุง ุงูุฑ ู ...\n",
      "[106/228] ุนูุงุฆ ู ฺุงุฆ ุจูุงุฆ\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุนูุงุฆ ุขูนฺพ ุณุงู ฺฉ ุงูพู ูุงูุง ูพุงูพุง ฺฉ ุจุช  ูพุงุฑ ุงูุฑ ูุนุตูู ุณ ุจฺ ุงุณ ฺฉ ูุงูุง ...\n",
      "[107/228] ุงููฺฉฺพุง ุฌูฺฏู\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุทุงุฑ ุงูพู ูุงู ฺฉ ฺฏฺพุฑ ุขุง ูุง ุชฺพุงู ุงูพู ุฑุดุช ฺฉ ุจู ุจฺพุงุฆูฺบ ฺฉ ุณุงุชฺพ ูุงู ฺฉ ุจุง...\n",
      "[108/228] ุงูุฑ ูุนุงู ูู ฺฏุฆ\n",
      "   โ 32 paragraphs extracted\n",
      "   ๐ Preview: ุฎูุด ฺฉ ูุงุฑ ูุฏ ฺฉ ูพุงุคฺบ ุฒูู ูพุฑ ูนฺฉ ูฺบ ุฑ ุชฺพุงุณ ูู ูฺบ ุข ุฑุง ุชฺพุง ฺฉ ู ุง...\n",
      "[109/228] ุขุฒุงุฏ  ุงฺฉ ูุนูุช\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: ุจุจูู ุงฺฉ ุดุฑุงุฑุช ุณุง ุจฺ ุชฺพุงุ ุฌุณ ุชุชูุงฺบ ุงูุฑ ูพุฑูุฏ ูพฺฉฺู ฺฉุง ุจุช ุดูู ุชฺพุงฺฏฺพุฑ ูุงู ุง...\n",
      "[110/228] ฺฺพููน ุณุฒุง\n",
      "   โ 40 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุบ ูุงุฑู ูฺบ ุณูุฏ ูุฑุบุงฺบ ุฏูุฑ ุณ ุฏฺฉฺพู ูฺบ ุจุช ุญุณู ููุธุฑ ูพุด ฺฉุฑ ุฑ ุชฺพฺบ ฺฺพ ...\n",
      "[111/228] ุจุงุฏุดุงุ ุดุฒุงุฏ ุงูุฑ ุดุฑุงุฑุช ุจฺพฺฺบ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ฺฉ ุงฺฉ ุจุงุฏุดุง ุงูุฑ ุงูู ฺฉุง ุจูนุง ุณููุงู ุชฺพุงุณููุงู ุฌุงููุฑูฺบ ฺฉ ุฏุฑ...\n",
      "[112/228] ฺฉู ุงูุฑ ุฎุฑฺฏูุด ฺฉ ุฏูุณุช\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุฌูฺฏู ูฺบ ุงฺฉ ุฏุฑุฎุช ูพุฑ ฺฉู ฺฉุง ฺฏฺพููุณูุง ุชฺพุงฺฉู ุจุช ูุณ ุฎูุด ุฑุช ุชฺพ ุงุณ ุฏุฑุฎุช ...\n",
      "[113/228] ุจุงุฏุฑ ฺูพุงูู ุงูุฑ ุขู ฺฉุง ุฏุฑุฎุช\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูุญู ฺฏูุฒุงุฑ ุขุจุงุฏ ฺฉุง ุขู ฺฉุง ุฏุฑุฎุช ุจุณุช ฺฉ ูพฺุงู ุจู ฺฺฉุง ุชฺพุง ุฏุฑุฎุช ู ุตุฑู ุณุง ูุฑุงู...\n",
      "[114/228] ูุณูุช ุตุจุฑ ุงูุฑ ุฐุงูุช ฺฉ ฺฉุงู\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ุงฺฉ ุบุฑุจ ุดุฎุต ุฑฺฏุณุชุงู ูฺบ ุณูุฑ ฺฉุฑ ุฑุง ุชฺพุง ุงุณ ุจุช ุจฺพูฺฉ ุงูุฑ ูพุงุณ...\n",
      "[115/228] ุฌุงุฏูุฆ ุณฺฉ\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุฏูุฑ ุฏุฑุงุฒ ฺฉ ูพุงฺูฺบ ูฺบ ุงฺฉ ฺฺพููนุง ุณุง ฺฏุงุคฺบ ุชฺพุงฺฺพููน ฺฺพููน ฺฏฺพุฑ ุชฺพุณุจุฒ ููุงุช ฺฉฺพ...\n",
      "[116/228] ูุง ููู ูพุฑุงู ุงุฏฺบ\n",
      "   โ 30 paragraphs extracted\n",
      "   ๐ Preview: โุงุฑ ุจฺพุฆุ ุงุจ ู ุจฺพ ุขุค ฺุงุฆุงูุชุธุงุฑ ฺฉุฑุช ฺฉุฑุช ูุฑ ุจุงู ูุฒุฏ ุณูุฏ ู ฺฏุฆ ฺบโ\n",
      "[117/228] ุจ ูุตูุฑ\n",
      "   โ 19 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฑ ูุณุงุฆ ฺฺุง ุงุจู ุนุฏูุงู ุงูพู ฺฏฺพุฑ ฺฉ ุจุงุบฺ ูฺบ ฺฉฺพฺ ูพูุฏูฺบ ฺฉ ฺฉุงููน ฺฺพุงููน ฺฉุฑ ุฑ...\n",
      "[118/228] ุงุชฺพ ูุฑุง ุณุงุชฺพ\n",
      "   โ 26 paragraphs extracted\n",
      "   ๐ Preview:  ฺฉุงู  ุงฺฉ ุฑู ฺฉ ุจฺ ฺฉุ ุฌุณ ฺฉุง ูุงู โูฺูโ ุชฺพุงูฺู ูุงฺบ ุจุงูพ ฺฉุง ฺฉูุง ูฺบ ูุง...\n",
      "[119/228] ฺฏุงุฆ ุงูุฑ ุจฺฉุฑ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ู ุงฺฉ ุจุช ุฎูุจุตูุฑุช ุฑ ุจฺพุฑ ฺุฑุงฺฏุง ุชฺพุ ุฌุงฺบ ุฎูุจุตูุฑุช ุงูุฑ ุตุงู ูุฏุงฺบ ุจุช ุชฺพฺบูุง...\n",
      "[120/228] ุญุงุถุฑ ุฏูุงุบ ุจุงุฏุฑ\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุฏู ุจฺพุฑ ฺฉ ุณุฎุช ูุญูุช ุงูุฑ ฺฉฺ ุฏฺพููพ ฺฉ ุจุนุฏ ุนุฏูุงู ฺฉู ฺฉฺฺพ ููุช ุฏููพุฑ ฺฉ ฺฉฺพุงู ฺฉ ูุฆ ูู...\n",
      "[121/228] ุชุชู ุณ ุฏูุณุช\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: ูุญูุฏ ุดุงู ฺฉ ุนูุฑ ุณุงุช ุณุงู ุชฺพุ ู ูุงูุฏู ฺฉุง ูุฑูุงูุจุฑุฏุงุฑ ุชฺพุงุงุณ ฺฉ ุนูุงู ุงุณ ฺฉุง ุดูุงุฑ...\n",
      "[122/228] ูุงุฏุงู ฺฉุง ุงูุฌุงู\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ฺฺพููน ุณ ุฌูฺฏู ูฺบ ุดุฑ ุจุงุฏุดุง ุญฺฉููุช ฺฉุฑุชุง ุชฺพุงุดุฑ ูุงุช ุฌูุฏุจุงุฒ ุงูุฑ ุจ ูููู ุชฺพุง...\n",
      "[123/228] ูุตุญุช ูพุฑ ุชูุฌ ุฏู\n",
      "   โ 23 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ุงฺฉ ฺูููน ุฌูู ฺฉ ุฏุงู ุฌูุน ฺฉุฑู ฺฉ ูุฆ ุงฺฉ ุฑุณุช ุณ ฺฏุฒุฑ ุฑ ุชฺพ...\n",
      "[124/228] ุฏู ุฌุงุณูุณ\n",
      "   โ 78 paragraphs extracted\n",
      "   ๐ Preview: ูุงู ุชู ุงู ฺฉุง ุชฺพุง ุณูนฺพ ูุฒุงฺฉุชุ ูฺฏุฑ ู ุชฺพ ุงฺฉ ูููน ุฌุณู ฺฉ ูุงูฺฉ ูุฌ ุชฺพ ฺฉ ู ุฑ...\n",
      "[125/228] ูพุฑูุฏ ฺฉุง ุชุญู\n",
      "   โ 38 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ฺฏุงุคฺบ ูฺบ ุฏูู ูุงู ฺฉุง ุงฺฉ ฺฉูุงุฑ ุฑุง ฺฉุฑุชุง ุชฺพุงุฏูู ุงูพู ฺฉุงู ูฺบ ุจุช ูุงุฑ ุชฺพุงู...\n",
      "[126/228] ุฑูฺฏุช\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: โุญุณู! ุชูฺบ ุณุฑ ูุงุตู ุงุณูนุงู ุฑูู ูฺบ ุจูุง ุฑ ฺบโฺูพฺุงุณ ู ฺฉูุฑ ุฌูุงุนุช ูฺบ ุฏุงุฎู ู ...\n",
      "[127/228] ุงุตู ุฏููุช\n",
      "   โ 3 paragraphs extracted\n",
      "   ๐ Preview: ุนุงุดุฑ ุงฺฉ ุจุช ุจฺ ุงู ฺฉูุฑ ูฺบ ููุฌูุฏ ุชฺพุงฺฉูุฑ ฺฉ ุนู ูุณุท ูฺบ ุงฺฉ ุจุช ุจฺุง ุฑุง ููุฌ...\n",
      "[128/228] ฺุงุฑ ุนุฌุจ ุจูฺบ\n",
      "   โ 24 paragraphs extracted\n",
      "   ๐ Preview: ุขุฌ ู โูู ุฎุงูุฏุงูโ ฺฉ ฺุงุฑ ุจููฺบ ุณ ุขูพ ฺฉ ููุงูุงุช ฺฉุฑูุง ุฑ ฺบ ฺุงุฑ ุจูฺบ ุจุช ู...\n",
      "[129/228] ุณุฑฺฉุณ ฺฉุง ฺฏฺพูฺุง\n",
      "   โ 18 paragraphs extracted\n",
      "   ๐ Preview: ุฌูฺฉ ุงฺฉ ฺฉู ุนูุฑ ฺฏฺพูฺุง ุชฺพุงุ ุฌุณ ู ุณุฑฺฉุณ ฺฉ ุฏูุง ูฺบ ุขูฺฉฺพ ฺฉฺพููุ ูฺฏุฑ ุงุณ ุณุฑฺฉุณ ูพุณูุฏ ...\n",
      "[130/228] ฺุชุงุ ูููฺ ุงูุฑ ฺฉุณุงู\n",
      "   โ 42 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏู ุฏู ุดฺฉุงุฑูฺบ ู ฺุช ฺฉู ุฏฺฉฺพุงู ุงุณ ฺฉุง ูพฺฺพุง ฺฉุฑู ูฺฏฺุชุง ุงู ุณ ุฌุงู ุจฺุง ฺฉุฑ...\n",
      "[131/228] ุงูุฏฺพุง ุณุงููพ ุงูุฑ ฺุงฺฉู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ฺฉ ุฌูู ูพูุฑ ฺฉ ุงฺฉ ฺฏุงุคฺบ ูฺบ ุงฺฉ ูุดูุฑ ฺุงฺฉู ุฑุชุง ุชฺพุงู ุงูพูุง ู...\n",
      "[132/228] ุดุฏ ฺฉ ูฺฉฺพ ฺฉุง ฺฺพุช\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูพุงุฑ ุจฺู! ุขูพ ูฺบ ุณ ุจุช ุณ ุจฺูฺบ ู ุดุฏ ฺฉุง ฺฺพุช ุชู ุฏฺฉฺพุง ู ฺฏุง ุฏุฑุงุตู  ููู ฺฉ ุฏ...\n",
      "[133/228] ุฏุงุฏุง ฺฉุง ฺฉฺพูููุง\n",
      "   โ 23 paragraphs extracted\n",
      "   ๐ Preview: ุฏุงุฏุง ุฌุงู ูฺบ ุงฺุงูฺฉ ฺู ุฌุงุฆฺบ ฺฏุ ุนุจุฏุงูู ู ุงุณุง ฺฉุจฺพ ุณูฺุง ุจฺพ ู ุชฺพุงุฏุณ ุฏู ูพู...\n",
      "[134/228] ุงฺฉ ุฏู ฺฉุง ุงุณุชุงุฏ\n",
      "   โ 23 paragraphs extracted\n",
      "   ๐ Preview: ูพุงูฺูฺบ ฺฉูุงุณ ฺฉ ูนฺุฑ ุฌุจ ุงูุฏุฑ ุฏุงุฎู ูุฆฺบ ุชู ูุงฺบ ฺฉุง ููุธุฑ ุบุต ุฏูุงู ฺฉ ูุฆ ฺฉุงู ุชฺพ...\n",
      "[135/228] ูฺบ ุงฺฉูุง ูฺบ ูฺบ\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ุงู ฺฉุง ุงุตู ูุงู ุชู ุจูุงู ุชฺพุงุ ูฺฉู ุณุจ ุงู ฺฉู ุจููููู ุจฺพุงุฆ ฺฉ ูุงู ุณ ูพฺฉุงุฑุช ุชฺพุจูููู...\n",
      "[136/228] ูพุฑุงูุง ุณฺฉ\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูฺบ ุดุด ฺฉ ุดูฺฉุณ ูฺบ ุฎุงููุด ูพฺุง ุชฺพุงุจูุจ ุณ ูฺฉูู ูุงู ุชุฒ ุฑูุดู ูุฌฺพ ูพุฑ ูพฺ ุฑ ุชฺพ...\n",
      "[137/228] ุนููููุฏ ุจูุฏุฑ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ ุ ฺฉุณ ุฌูฺฏู ูฺบ ุงฺฉ ุธุงูู ุดุฑ ุฑุชุง ุชฺพุงุฑ ุฑูุฒ ุฏู ุชู ุฌุงููุฑูฺบ ฺฉู ...\n",
      "[138/228] ูุนุฏ\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุฑุงุญู ุฑูุช ูุฆ ุงูพู ุฏุงุฏ ฺฉ ฺฉูุฑ ูฺบ ุฏุงุฎู ูุงุงุณ ุฑูุชุง ุฏฺฉฺพ ุงุณ ฺฉ ุฏุงุฏ ุณุฎุช ูพุฑุด...\n",
      "[139/228] ุฑุณู ู ุฑูุงุช\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: โุงุฑ! ุงุฑ ุชู ูุฑ ุงููพุฑ ุณ ฺฏุฒุฑ ฺฏุฆ ุจฺพุงุฆ! ุงุจ ูุงูพุณ ุข ฺฉุฑ ุฏูุจุงุฑ ฺฏุฒุฑูุ ูุฑู ูุฑุง ูุฏ ...\n",
      "[140/228] ูฺบ ุจุฒุฏู ูฺบ\n",
      "   โ 32 paragraphs extracted\n",
      "   ๐ Preview: ุดุฒุงุฏ ฺฏุงุคฺบ ฺฉุง ุณุจ ุณ ูฺฉูุง ูฺฺฉุง ุณูุฌฺพุง ุฌุงุชุง ุชฺพุงุงุณ ฺฉ ูุงูุฏ ุจุดุฑ ุงุญูุฏ ุทูู ุนุฑุต ุชฺฉ ฺฏ...\n",
      "[141/228] ฺฺพููน ุจฺ ุจฺุง ูุตู\n",
      "   โ 21 paragraphs extracted\n",
      "   ๐ Preview: ูุตุฑุช ฺฉ ุชู ุจูน ุงุญูุฏุ ุนุจุฏุงูู ุงูุฑ ููุงู ุชฺพุงู ฺฉ ุดูุฑ ุงฺฉ ูุฌ ฺฉููพู ูฺบ ููุงุฒู ุช...\n",
      "[142/228] ูุชูุงุฒู ุบุฐุง ุถุฑูุฑ \n",
      "   โ 13 paragraphs extracted\n",
      "   ๐ Preview: โููุง! ุขุฌ ฺฉุง ูพฺฉุงุง ุโ ุนุงุฆุด ุณฺฉูู ุณ ูุงูพุณ ุขุฆ ุชู ุขุช  ุจ ุชุงุจ ุณ ูพูฺฺพุงุ ุงุณ ุด...\n",
      "[143/228] ุงฺฉ ุชฺพ ุจฺฺพุง\n",
      "   โ 20 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุชฺพ ุจฺฺพุง ุงูุฑ ุงุณ ฺฉุง ุงฺฉ ุจูนุง ุจฺพ ุชฺพุงุจูนุง ุจุช ุฏูุฑ ุฑุชุง ุชฺพุงุจฺฺพุง ุงฺฉู ุชฺพ ุง...\n",
      "[144/228] ูุงฺบ ฺฉ ุฏุนุง\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุดุฑ ูฺบ ุงฺฉ ุบุฑุจ ุจู ุนูุฑุช ุฑุช ุชฺพุงุณ ฺฉ ุฏู ุจูน ุชฺพู ุบุฑุจ ุนูุฑุช ูุญูุชุ ูุฒุฏ...\n",
      "[145/228] ุดุฑุงุฑุช ฺฉูุง ุงูุฑ ุฑุญู ุฏู ฺฺุง\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ฺฺุง ุจ ูู ูฺบ ุงฺฉ ุดุงูพุฑ ู ุฌุณ ูฺบ ุชฺพูฺ ุณ ูพฺฉ ูุฆ ฺุงูููฺบ ฺฉ ุฏุงู ุชฺพ ุงูพู ฺฏฺพ...\n",
      "[146/228] ุตุจุฑ ฺฉุง ุงูุนุงู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงุญูุฏ ูพู ุฌูุงุนุช ฺฉุง ููน ฺฉฺพูน ุงูุฑ ุดุฑุงุฑุช ุณุง ุจฺ ุชฺพุงุงุณ ฺฉ ุงูุฏุฑ ุงฺฉ ุจุฑ ุนุงุฏุช ุชฺพ ฺฉ ู...\n",
      "[147/228] ุงูู ุณุจ ฺฉุง ุฑุงุฒู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ูุฑูพูุฑ ฺฉ ุงฺฉ ฺฺพููน ุณ ฺฏุงุคฺบ ูฺบ ุงุญูุฏ ุตุงุญุจ ุงูพู ฺฉูนุง ูฺบ ุงฺฉู ุฑุช ุชฺพุงู ฺฉ ุงฺฉ...\n",
      "[148/228] ุฑูฺฏ ุจุฑูฺฏ ุชุชูุงฺบ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุชุชูุงฺบ ฺฉู ู ุจุด ุฏูุง ฺฉ ุฑ ููฺฉ ูฺบ ูพุงุฆ ุฌุงุช ฺบุงู ฺฉ ุจ ุดูุงุฑ ุฑูฺฏ ุงูุฑ ูุณูฺบ ฺบ...\n",
      "[149/228] ุชฺพุงูุณ ุงูุฑ ฺฉููุฑ\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ุขุฌ ุชฺพุงูุณ ุงูุฑ ฺฉููุฑ ฺฉ ุงุณฺฉูู ูฺบ ุงฺฉ ุจฺ ุณุงูุงู ุชูุฑุจ ุชฺพุงุชูุงู ุณ ุงู ฺฉุง ฺุฑุงุฆูุฑ ...\n",
      "[150/228] ุงฺฉ ุชฺพ ูุฑูู\n",
      "   โ 25 paragraphs extracted\n",
      "   ๐ Preview:  ฺฉุงู ุณูุ ฺฺฺพ ุณู ุณุงู ูพุฑุงู ุงุณ ููุช ุงุชู ุชุฑู ูฺบ ูุฆ ุชฺพุ ุฌุชู ุขุฌ ู ฺฺฉ...\n",
      "\n",
      "   ๐ Restarting browser (processed 150 stories)...\n",
      "\n",
      "[151/228] ุณุงุช ูพุฑุงฺบ ุณุงุช ุณูุงฺบ\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ูพุฑุณุชุงู ูฺบ 7 ุจ ุญุฏ ุฎูุจุตูุฑุช ูพุฑุงฺบ ุฑุช ุชฺพฺบู ุขูพุณ ูฺบ ุจุช ุงฺฺพ ุณูุงฺบ ุจฺพ ุช...\n",
      "[152/228] ุฏูุณุช ู ุชู ุงุณ (ุขุฎุฑ ุญุต)\n",
      "   โ 13 paragraphs extracted\n",
      "   ๐ Preview: ุนูุฑ ุจููุง โุชุช ุชุชุ ุชู ุชู ุงุณุง ูฺบ ฺฉุฑ ุณฺฉุชุ ูุฌฺพ ฺฺพูฺ ุฏูโ ุฌูฺฏ ุบุต ุณ ุจููุง โุชู ...\n",
      "[153/228] ุฏูุณุช ู ุชู ุงุณ (ูพูุง ุญุต)\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุงูุฑุงู ุงูุฑ ุนูุฑ ฺฉ ุฏูุณุช ูพูุฑ ฺฉุงููู ูฺบ ูุดูุฑ ุชฺพุฏูููฺบ ุงฺฉ ุฏูุณุฑ ูพุฑ ุฌุงู ฺฺพฺฺฉุช ...\n",
      "[154/228] ุดุฑุฑ ุจฺพุงูู\n",
      "   โ 29 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฑ ุจฺพุฑ ุฌูฺฏู ูฺบ ุจุช ุณุงุฑ ุจฺพุงูู ุฑุง ฺฉุฑุช ุชฺพุ ุฌุงฺบ ููนฺพ ูพุงู ฺฉ ฺุดูุ ุณุฑุฎ ...\n",
      "[155/228] ฺุงูุฏ ูพุฑ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ฺุงูุฏ ูพุฑ ุงฺฉ ุฎูุจุตูุฑุช ุงูุฑ ูพูุฑุงุณุฑุงุฑ ูุฎููู ุชฺพ ุฌู ฺุงูุฏ ูพุฑ ุฑุช ุชฺพุงุณ ฺฉ ุณูุฏ ุฑุดู...\n",
      "[156/228] ุงฺฺพ ุจฺ ูฺบ ูฺุช\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุฑุงุฌู ุงูุฑ ุดุงู ูุด ฺฉุณ ู ฺฉุณ ุจุงุช ูพุฑ ุฌฺพฺฏฺุง ฺฉุฑุช ูุธุฑ ุขุช ุชฺพุงู ุฏูููฺบ ฺฉ ฺฏฺพุฑ ูุงู...\n",
      "[157/228] ูุฏุฏ ฺฉุฑูุง ุณฺฉฺพู\n",
      "   โ 20 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุง ูุงู ุดูุง  ุงูุฑ ุตุงุฆู ูุฑ ฺฺุง ุฒุงุฏ ุจู ู ุงฺฉ  ฺฏฺพุฑ ูฺบ ุฑุช ุชฺพู ุจุช...\n",
      "[158/228] ุจููฺบ ฺฉุง ฺฏฺพุฑ\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ฺฉ ูุงุฑ ูุงู ุงฺฉ ุจฺุ ุงฺฉ ฺฺพููน ุณ ุดุฑ ูฺบ ุงูพู ุงู ุงุจู ฺฉ ุณ...\n",
      "[159/228] ุงููฺฉฺพุง ุฎุท\n",
      "   โ 33 paragraphs extracted\n",
      "   ๐ Preview: ุตุฏุฑู ูููฺฉุช ุงฺุงูฺฉ ฺฉุฑุณ ุณ ุงููนฺพ ฺฉฺพฺ ูุฆุงู ฺฉ ฺุฑ ูพุฑ ูพุฑุดุงู ฺฉ ุขุซุงุฑ ููุงุงฺบ ุชฺพ...\n",
      "[160/228] ุงุตู ุงูุฑ ูุณู ฺฉ ูพฺุงู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุฌูฺฏู ูฺบ ุงฺฉ ุจฺฉุฑ ุจุช ุจ ูฺฉุฑ ุณ ุงูุฏฺพุฑ ุงูุฏฺพุฑ ฺฏฺพูู ุฑ ุชฺพุงฺฉ ฺฉู ฺฉู ุจุช ุญุฑ...\n",
      "[161/228] ุฏูุณุช ู ุฌู ูุตุจุช ูฺบ ฺฉุงู ุขุฆ\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุฌูฺฏู ูฺบ ุงฺฉ ุฏุฑุฎุช ูพุฑ ุฏู ฺฉุจูุชุฑ ูุฑ ุงูุฑ ูุงุฏ ุฑุช ุชฺพู ุฏูููฺบ ูุณ ุฎูุด ุฒูุฏฺฏ ...\n",
      "[162/228] ูููุง ุจุฎุด\n",
      "   โ 34 paragraphs extracted\n",
      "   ๐ Preview: ูููุง ุจุฎุด ุงูุฑ ุงุณ ฺฉ ุจู ุงฺฉ ฺฏุงุคฺบ ูฺบ ุฑุช ุชฺพู ุจฺูฺบ ฺฉ ุณุงุชฺพ ุตุจุฑ ู ุดฺฉุฑ ุณ ุฒูุฏฺฏ...\n",
      "[163/228] ุฎุงู ูพูุฑ\n",
      "   โ 22 paragraphs extracted\n",
      "   ๐ Preview: ุตุจุญ ฺฉ ูพุงูฺ ุจุฌ ุฑ ุชฺพุฎุฏุง ุจุฎุด ู ุงูพู ฺฉูุฒูุฑุ ูุงุบุฑ ุฌุณู ฺฉู ูุช ฺฉุฑ ฺฉ ฺุงุฑูพุงุฆ ุณ ู...\n",
      "[164/228] ฺูุฑ ฺฉูู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุงุญูุฑ ุงูุฑ ูุงุฑุณ ุฏูููฺบ ุจฺพุงุฆ ุชฺพุขุฌ ู ุฏูููฺบ ุงูพู ุงุจู ุฌุงู ุงูุฑ ุงู ฺฉ ุฏูุณุชูฺบ ฺฉ ุณุงุชฺพ ...\n",
      "[165/228] ุบุฑูุฑ ฺฉ ุณุฒุง\n",
      "   โ 18 paragraphs extracted\n",
      "   ๐ Preview: ุจุงุฑ ุณุงู ูุงู ุณุงุชูฺบ ฺฉูุงุณ ฺฉ ุทุงูุจ ุชฺพู ุจุช ุฐู ุงูุฑ ุงฺฺพ ุฏู ฺฉ ูุงูฺฉ ุชฺพุงูพู...\n",
      "[166/228] ฺฺพููนุง ูพฺพูู\n",
      "   โ 13 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุจุงุบ ูฺบ ุจุช ุณ ูพูุฏ ุงูุฑ ุฏุฑุฎุช ุชฺพุ ุฌู ูพุฑ ุฑ ุทุฑุญ ฺฉ ูพฺพูู ุงูุฑ ูพฺพู ูฺฏ ูุฆ ุชฺพ...\n",
      "[167/228] ูููู ฺฉ ุจุงุฏุฑ\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ูููู ุงฺฉ ุจฺพุงูู ฺฉุง ุจฺ ุชฺพุงุ ุฌู ุงูพู ุงู ุงุจู ุงูุฑ ุฏู ฺฺพููน ุจฺพุงุฆูฺบ ฺฉ ุณุงุชฺพ ูพุงฺ ุฌู...\n",
      "[168/228] ฺูุจุช ฺฉู ุชูฺฉ ฺฉุง ุณุงุฑุง\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ุจุฑุณุงุช ูฺบ ููุณูุง ุฏฺพุงุฑ ุจุงุฑุด ูุฆุฌูฺฏู ฺฉูุงุฑ ุฑุช ฺฉ ุงฺฉ ูนู ูพ ููุฌูุฏ ฺูููนูฺบ ฺฉ ุจ...\n",
      "[169/228] ุณุฒุง ฺฉุง ุจุฏู\n",
      "   โ 28 paragraphs extracted\n",
      "   ๐ Preview: ุฏุณ ุณุงู ูููุ ูุฑุญุงู ุตุงุญุจ ฺฉุง ุงฺฉููุชุง ุงูุฑ ูุงฺูุง ุจูนุง ุชฺพุงุ ุฌุณ ุฏูุง ฺฉ ุฑ ุณููุช ุงูุฑ ...\n",
      "[170/228] ุดุงู ฺฉุง ุจฺพููุง\n",
      "   โ 35 paragraphs extracted\n",
      "   ๐ Preview: โูุฌฺพ ุขุฆ ููู ฺุงโ ุฑุงุช ฺฉู ฺฉฺพุงู ฺฉ ุฏูุฑุงู ูุฒูู ุจููุง\n",
      "[171/228] ุชุฐุจ ฺฉ ูพฺุงู\n",
      "   โ 20 paragraphs extracted\n",
      "   ๐ Preview: โุฎุงู ุฌุงู!โ ฺฉุณ ฺฉ ููนฺพ ุณ ุขูุงุฒ ุณูุงุฆ ุฏุ ูฺฉู ูฺบ ฺฉฺฺพ ู ุจูู ุงูุฑ ุจุฏุณุชูุฑ ุตุญู ฺฉ...\n",
      "[172/228] ูุงูฺ ฺฉ ุณุฒุง\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุฌูฺฏู ูฺบ ุฏู ุจูุงฺบ ุฑุช ุชฺพฺบุงฺฉ ุจู ฺฉุงู ุงูุฑ ุฏูุณุฑ ุงูุฑูุฌ ุชฺพุฏูููฺบ ุขูพุณ ูฺบ ุง...\n",
      "[173/228] ุงฺฉ ุงูุฑ ูููุน\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุธูุฑ ุจุช  ุงฺฺพุง ุงูุฑ ูุฑูุงูุจุฑุฏุงุฑ ุจฺ ุชฺพุงุ ูฺฏุฑ ุฌุจ ุณ ุงุณ ฺฉู ุณุงูฺฏุฑ ูพุฑ ููุจุงุฆู ฺฉุง ุชุญู...\n",
      "[174/228] ุชุฏุงุจุฑ ุฑุงุญุชู ุงูุฒุง\n",
      "   โ 20 paragraphs extracted\n",
      "   ๐ Preview: ุงุญูุฏ ุงูุฑ ูุณู ฺฉ ููนุฑฺฉ ฺฉ ุงูุชุญุงูุงุช ูู ูุงู ุชฺพุฏูููฺบ ูู ฺฉุฑ ูพฺฺพุช ุชฺพฺฉุจฺพ ุงุญูุฏ...\n",
      "[175/228] ุฌุงฺบ ุจ ูุจ ููุงุ ุฌุงู ูฺบ ุฌุงู ุขูุง\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงุณูุงุก ุจุช ูพุงุฑ ุจฺ ุชฺพุงุณ ูพฺฺพุงุฆ ฺฉ ุณุงุชฺพ ุณุงุชฺพ ูพูุฏ ูฺฏุงู ฺฉุง ุจฺพ ุจุช ุดูู ุชฺพุง ุงู...\n",
      "[176/228] ุขุฒุงุฏ\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ุฑุถุง ุตุงุญุจ ฺฉ ฺฏฺพุฑ ฺฉ ฺฺพุช ูพุฑ ุงฺฉ ุขุณูนุฑูู ุทูุทูฺบ ฺฉุง ุฌูฺุง ุชฺพุงุฑุถุง ุตุงุญุจ ุฑ ุงุชูุงุฑ ฺฉู ุงู...\n",
      "[177/228] ุณุฏุณ ฺฉุง ุจุงุบ\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุณุฏุณ ฺฏุงุคฺบ ฺฉุง ุจุช ุดุฑุฑ ูฺฺฉุง ุชฺพุงุงฺฉ ุฏู ุณุฏุณ ู ฺฏุงุคฺบ ฺฉ ุจุงุฑ ุงฺฉ ุจุช ุฑุง ุจฺพุฑุง ุฏุฑุฎุช...\n",
      "[178/228] ูููฺ ุงูุฑ ฺู ฺฉ ุฏูุณุช\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ุนุฒุฒ ุฏูุณุชู! ุจุช ูพู ฺฉ ุจุงุช  ุงฺฉ ูููฺ ุงฺฉ ฺู ฺฉ ุณู ุจู ฺฏุฆุฏูููฺบ ูฺบ ุฏูุณุช...\n",
      "[179/228] ุดุงู ุณ ูพู\n",
      "   โ 19 paragraphs extracted\n",
      "   ๐ Preview: โุนุชู ุจูนุง! ูุงุฑ ฺฉุงูุฌ ฺฉุง ุงูุตูู  ฺฉ ู ูุฑุณูน ุงุฆุฑ ฺฉ ุจูุฑฺ ฺฉุง ุฏุงุฎูุ ุงุณ ุทูุจ ฺฉ...\n",
      "[180/228] ุฐู ุฏุงุฑ ูนูฺฉู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ฺฏุงุคฺบ ูฺบ ุงฺฉ ุขุฏู ุงูพู ุจูน ูนูฺฉู ฺฉ ุณุงุชฺพ ุฑุชุง ุชฺพุงูนูฺฉู ุงฺฉ ุจุช  ูพุงุฑุงุ ุฎูุจ...\n",
      "[181/228] ฺูฺฉูุง ุฎุทุฑ\n",
      "   โ 15 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุง ุงฺฉููุชุง ุจูนุง ุซุงูุจ ุณุงูุงู ุงูุชุญุงู ูฺบ ุจุช ฺฉู ููุจุฑ ู ุณฺฉุง ุชฺพุงูุฌฺพ ูุฌ ฺฉุง ุงูุฏุงุฒ...\n",
      "[182/228] ุฌูุฏ ุจุงุฒ ฺฉุง ุงูุฌุงู\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุนูุฑุงู ุงูุฑ ุงุญูุฏ ุงูพู ุงู ุงุจูุ ุฏุงุฏุง ุฏุงุฏ ฺฉ ุณุงุชฺพ ุงฺฉ ฺฺพููน ุณ ูุตุจ ูฺบ ุฑุช ุชฺพุนู...\n",
      "[183/228] ฺฉูุงุช ุดุนุงุฑ\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุงุฒ ู ุจุฑฺฏุฑ ุฎุฑุฏุง ุงูุฑ ุณุงุฆฺฉู ูพุฑ ฺฏฺพุฑ ฺฉ ุฌุงูุจ ุฑูุงู ู ฺฏุงุงู ู ูุฑุงุฒ ฺฉู ุจุฑฺฏุฑ ู...\n",
      "[184/228] ุงูพูุง ฺฉุงู ุฎูุฏ ฺฉุฑู\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ฺฉุณ ุจุงุบ ูฺบ ุงฺฉ ฺฉุจูุชุฑ ู ุงูพูุง ุขุดุงู ุจูุงุง ูุง ุชฺพุงุฌุณ ูฺบ ู ุฏู ุจฺพุฑ ุงูพู ุจฺูฺบ ฺฉู...\n",
      "[185/228] ุชู ููุช\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview:  ฺฏุฆ ููุชูฺบ ฺฉ ุจุงุช ุ ุฎุงูุฏ ูุงู ูุฑูุงฺบ ุจุฑุฏุงุฑ ุจฺ ุงูพู ูุงู ฺฉ ุณุงุชฺพ ุงฺฉ ฺฺพููน ุณ...\n",
      "[186/228] ุฎูุงุดู ูพุฑูุงุฒ\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ูุฑุญุงู ุจฺพุงุฆ ุฌุจ ูุถุง ูฺบ ูพุฑูุงุฒ ฺฉุฑุช ูุฆ ูพุฑูุฏูฺบ ฺฉู ุฏฺฉฺพุช ุชู ุงู ฺฉ ุฏู ูฺบ ุจฺพ ุงฺู...\n",
      "[187/228] ุงูุณุงู ฺฉ ุนูุฑ\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ูพุงุฑ ุจฺู! ุฌุจ ุฎุฏุง ู ุฏูุง ุจูุงุฆ ุงูุฑ ุงุณ ูฺบ ุจุณู ฺฉ ูุฆ ุงูุณุงูุฺููพุงุฆ ูพุฑูุฏ ุงูุฑ ุฏู...\n",
      "[188/228] ูุช ุจุฎุฑ\n",
      "   โ 39 paragraphs extracted\n",
      "   ๐ Preview: ุจุณ ูฺบ ุจูนฺพุช  ูุจู ู ุงูพู ุฌุจ ฺฉู ูนูนูู ฺฉุฑ ุชุณู ฺฉ ฺฉ ุงุณ ฺฉ ุฌุจ ูฺบ ุจูนูุง ููุฌูุฏ...\n",
      "[189/228] ุฑุงุฆ\n",
      "   โ 35 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉูู ููนุฑฺฉ ฺฉุง ุทุงูุจ ุนูู ุชฺพุงุ ูฺฉู ุงูพู ูพฺฺพุงุฆ ุงูุฑ ูุงูุฏู ุงูุฑ ุงุณุงุชุฐ ฺฉุง ุงุญุชุฑุงู ฺฉุฑู...\n",
      "[190/228] ูพฺพูููฺบ ูุงูุง ุฑุงุณุช\n",
      "   โ 29 paragraphs extracted\n",
      "   ๐ Preview: ฺฺุง ุฑุญูุช ูุญู ุจฺพุฑ ฺฉ ฺฺุง ุชฺพฺฺพููน ุชู ฺฺพููนุ ุงูู ฺฉ ุนูุฑ ุณ ุจฺ ุจฺพ ุงูฺพฺบ ฺฺุง  ...\n",
      "[191/228] ูุงูุฑูุงู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุณูุน ุงูุฑ ุณูุฑ ุฏูููฺบ ุจฺพุงุฆ ุชฺพุงู ฺฉ ูุงู ฺฉุง ฺฏฺพุฑ ุฌูฺฏู ฺฉ ูุฑุจ ุชฺพุงู ุฑ ุณุงู ฺฺพูนู...\n",
      "[192/228] ุนููููุฏ ูฺฉฺุงุฑุง\n",
      "   โ 5 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุจุงุฏุดุง ู ููฺฉ ุจฺพุฑ ูฺบ ฺฉฺฺพ ุนุฌุจ ุณุง ุงุนูุงู ฺฉุฑูุงุง ฺฉ ุฌู ุจฺพ ุจุงุฏุดุง ฺฉู ุงูพู ุงุณ...\n",
      "[193/228] ูุฑ ูพุฑูุงุฒ\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฆฺบ ุงูพู ุฒูุฑ ูพุฑ ูุฌฺพ ุงูฺุงุฆ ูุฆ ุฌุง ุฑ ุชฺพฺบ ูฺฉ ูฺฉ ูพุฑูุงุฒ ูุฌฺพ ุจุช ุงฺฺพ ...\n",
      "[194/228] ูุฒุฏูุฑ ูพุฑ ุธูู\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ูุจุดุฑ ุงฺฉ ูฺฉูนุฑ ูฺบ ฺฉุงู ฺฉุฑุชุง ุชฺพุงู ุงูพู ฺฏฺพุฑ ฺฉุง ูุงุญุฏ ฺฉูุงู ูุงูุง ุชฺพุงุจู ุจฺุ ุจู...\n",
      "[195/228] ุงุนุชูุงุฏ\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ูุงู ุงฺฉ ุจุช ุงฺฺพ ุจฺ ุชฺพู ูุงฺบ ุจุงูพ ฺฉ ูุฑูุงูุจุฑุฏุงุฑ ุชฺพุฑ ุงฺฉ ฺฉุง ฺฉุง ูุงูุช ุชฺพุ...\n",
      "[196/228] ุจฺพฺฺบ ุงูุฑ ุจฺพฺ\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุณุงุชฺพู! ุงฺฉ ุฌูฺฏู ฺฉ ุจฺ ุญุต ูพุฑ ุจฺพฺูฺบ ฺฉ ุญฺฉููุช ูุงุฆู ุชฺพ ุงูุฑ ุฏูุณุฑ ฺฺพููน ุณ ุญุต ...\n",
      "[197/228] ูุฌุจูุฑ ูุฌุฑู\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ฺฺุง ุฑุญูู ุงฺฉ ูุฑุจุงู ุขุฏู ุชฺพุงู ฺฉ ูพุงุณ ุจุช ุณุงุฑ ุจฺฉุฑุงฺบ ุชฺพฺบุงู ฺฉุง ุจูนุง ุฑูุฒุงู ุง...\n",
      "[198/228] ูพูุฑุงุณุฑุงุฑ ูฺฺฉุง\n",
      "   โ 8 paragraphs extracted\n",
      "   ๐ Preview: ุงู ู ฺฏฺพุฑ ฺฉ ุณุงูู ุจุงุบฺ ูฺบ ุงฺฉ ูพฺพูู ูพุฑ ุชุชู ุจูนฺพ ุฏฺฉฺพุงุณ ู ฺฏฺพุฑ ูฺบ ุฌฺพุงูฺฉ...\n",
      "[199/228] ุญูุงู ุฑุฒู ุงฺฉ ุนุจุงุฏุช\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุณูู ุงฺฉ ุงูุงูุฏุงุฑ ุงูุณุงู ุชฺพุงุ ุฌู ุงฺฉ ฺฺพููน ุณ ูุตุจ ูฺบ ุฑุชุง ุชฺพุงุงุณ ฺฉ ูพุงุณ ฺฉฺฺพ ุจฺพ...\n",
      "[200/228] ููู ฺฉู ูู ุฌฺพููน ฺฉ ุณุฒุง\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ูพุงุฑ ุจฺู! ุฏูุฑ ฺฉุณ ุฌูฺฏู ูฺบ ุจููฺบ ฺฉุง ุงฺฉ ฺฏุฑู ุฑุง ฺฉุฑุชุง ุชฺพุงุงู ุณุจ ฺฉ ุขูพุณ ูฺบ ฺฏุฑ...\n",
      "\n",
      "   ๐ Restarting browser (processed 200 stories)...\n",
      "\n",
      "[201/228] ููุช ฺฉ ุงูุช\n",
      "   โ 4 paragraphs extracted\n",
      "   ๐ Preview: ุขุฌ ฺฉู ฺฉ ุฒูุฏฺฏ ุจฺ ูุตุฑูู ุฒูุฏฺฏ ุฑ ุขุฏู ฺฉุงููฺบ ฺฉ ฺฉุซุฑุช ฺฉ ุดฺฉุงุช ฺฉุฑุชุง  ููุฑ...\n",
      "[202/228] ุทูุท ฺฉ ุจุงุช\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ฺฉุณุงู ฺฉ ฺฏฺพุฑ ูฺบ ุงฺฉ ฺฺุง ูพู ูุฆ ุชฺพู ุงูุณุงููฺบ ฺฉ ุจูู ุฌุงูุช ุชฺพ ุงูุฑ ุฌู ฺฉฺ...\n",
      "[203/228] ุฏูุณุช ู ุฌู ูุตุจุช ูฺบ ฺฉุงู ุขุฆ\n",
      "   โ 21 paragraphs extracted\n",
      "   ๐ Preview: ฺฏุฑููฺบ ฺฉ ุงฺฉ ุชูพุช ุฏููพุฑ ูฺบ ุฌุจ ุณูุฑุฌ ุขฺฏ ุจุฑุณุง ุฑุง ุชฺพุง ุงูุฑ ููฺฉ ุชฺพูพฺ ุณูฺฉฺพ ูพุชฺพูฺบ...\n",
      "[204/228] ุนูุงุจ ุงูุฑ ูฺฉฺ\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุนูุงุจ ุจุงุฏููฺบ ฺฉู ฺุฑุชุง ูุง ฺฉู ูุงู ฺฉ ฺููนูฺบ ูพุฑ ูพูฺุง ุงูุฑ ุงู ฺฉุง ฺฺฉุฑ ูฺฏุง ฺฉุฑ ุงฺฉ...\n",
      "[205/228] ุฎูุฏุฏุงุฑ ูฺฺฉ\n",
      "   โ 23 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฑ ูุญุถ ุชุฑ ุณุงู ฺฉ ุชฺพุ ูฺฏุฑ ุงููุฑู ุฎุงู ุฏุงุฑ ูฺบ ูุงุฑ ู ฺฺฉ ุชฺพุ ฺฉููฺฉ ุงุณ ู...\n",
      "[206/228] ุฎู ุจูนู\n",
      "   โ 9 paragraphs extracted\n",
      "   ๐ Preview: ุฐฺฉุ ุฒุฏ ุงูุฑ ุงุญูุฑ ุงุณฺฉูู ฺฉ ฺฺพูน ฺฉ ุจุนุฏ ุญุณุจู ูุนููู ุงฺฉูนฺพ ฺฏฺพุฑ ูุงูพุณ ุข ุฑ ุชฺพ ฺฉ ุงฺ...\n",
      "[207/228] ุงุญุณุงุณ\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุนูุฑ ุงูุฑ ุงุฏ ฺฉู ุณุฑ ฺฉุฑู ฺฉุง ุจุช ุดูู ุชฺพุงุงุชูุงุฑ ฺฉ ฺฺพูน ุชฺพุฏูููฺบ ู ุฌูฺฏู ุฌุง ฺฉุฑ ฺฏฺพ...\n",
      "[208/228] ฺฉุงููนูฺบ ฺฉุง ุฌูุงุจ\n",
      "   โ 37 paragraphs extracted\n",
      "   ๐ Preview: ูุฑ ฺฉูุงุฑ ุจุฑฺฏุฏ ฺฉ ูพฺ ูพุฑ ุฌุงฺบ ูพุฑูุฏูฺบ ู ุงูพู ุขุดุงู ุจูุงุฆ ุชฺพุ ูฺบ ุงูุณุงููฺบ ู ุจ...\n",
      "[209/228] ุฑููน ุงูุฑ ููุณู\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ุจุงุฏุดุง ููฺฉ ูุนุงููุงุช ูฺบ ุงูพู ุงู ุณุงุช ุฏุฑุจุงุฑูฺบ ุณ ูุดูุฑ ฺฉุฑุชุงุ ุฌูฺบ ู ุจุช ุนูู ููุฏ...\n",
      "[210/228] ุชู ุจุงุฏุฑ\n",
      "   โ 25 paragraphs extracted\n",
      "   ๐ Preview: ูุงุฑุง ฺฏุงุคฺบ ุณูุฏฺพ ฺฉ ุงฺฉ ุฏูุฑ ุฏุฑุงุฒ ููุงู ูพุฑ ูุงูุน ุชฺพุงุงฺฉ ุทุฑู ุฑุชูุง ูุฏุงู ุชู ุฏูุณุฑ ุท...\n",
      "[211/228] ฺฉุงุง ูพููน\n",
      "   โ 13 paragraphs extracted\n",
      "   ๐ Preview: ูุฑ ุจฺูพู ฺฉ ุจุงุช ูุงุฑ ูพฺูุณ ูฺบ ุงฺฉ ุชู ูฺฺฉุง ุตูุฏุฑ ุงูพู ฺฺุง ฺฉ ูพุงุณ ุฑุชุง ุชฺพุงูพ...\n",
      "[212/228] ฺูู ุฎุงู\n",
      "   โ 28 paragraphs extracted\n",
      "   ๐ Preview: ฺูู ุฎุงู ูููุฑุฏ ุทุจุนุช ฺฉ ูุงูฺฉ ุชฺพฺบูุงู ฺฉ ุทุฑุญ ุจุงุบ ู ุจุงุฑ ุดุฎุตุช ูู ฺฉ ุจุงุนุซ ู ู...\n",
      "[213/228] ููุชูฺบ ฺฉุง ุงุฑ\n",
      "   โ 10 paragraphs extracted\n",
      "   ๐ Preview: ูพุฑุงู ููุชูฺบ ฺฉ ุจุงุช ุงฺฉ ฺฏุงุคฺบ ูฺบ ุงฺฉ ุจู ุงูพู ุจูน ุงูุดุงฺบ ุงูุฑ ุจูน ุงูุชุฎุงุฑ ฺฉ ุณ...\n",
      "[214/228] ุจููน ฺฉุงฺบ ฺฉฺพู ฺฏุง\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุจููน ุงฺฉ ุจูุฏุฑ ุชฺพุง ุฌู ุงูพู ูุงฺบ ุจุงูพ ฺฉ ุณุงุชฺพ ุฌูฺฏู ูฺบ ุฑุชุง ุชฺพุงู ุชู ุจฺพุงุฆ ุชฺพุจููน...\n",
      "[215/228] ุฌฺพู ูพ ุขุฆ ุงุชฺพ\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุจุช ุนุฑุต ูพู ฺฉ ุจุงุช  ฺฉ ุขุจุงุฏ ุณ ุฏูุฑ ุงฺฉ ูพุฑุงูุง ูุฑุงู ฺฏุงุคฺบ ุชฺพุงุงุณ ูฺบ ฺฏฺพุฑุ ฺฏู...\n",
      "[216/228] ุจูู ูุฑ ูฺฺพู (ุขุฎุฑ ุญุต)\n",
      "   โ 14 paragraphs extracted\n",
      "   ๐ Preview: ุงุณ ู ุบูุฑ ฺฉุง ุชู  ุขูุงุฒ ูฺฺพู ููุง ฺุฒ ุณ ุข ุฑ ุชฺพุงุณ ูุญุณูุณ ูุชุง ุชฺพุง ฺฉ ุฌุณ ...\n",
      "[217/228] ุจูู ูุฑ ูฺฺพู (ูพูุง ุญุต)\n",
      "   โ 7 paragraphs extracted\n",
      "   ๐ Preview: ุชููุฑ ู ุงฺฏู ฺูุจุณ ฺฏฺพููนูฺบ ฺฉ ูุฆ ุถุฑูุฑุช ฺฉ ุฑ ฺุฒ ูุงฺบ ฺฉ ุจุณุชุฑ ฺฉ ูพุงุณ ุงฺฉ ูุฒ ูพุฑ ...\n",
      "[218/228] ููุช ฺฉุง ุฌูุงุจ\n",
      "   โ 26 paragraphs extracted\n",
      "   ๐ Preview: ุงูุจุงู ุตุงุญุจ ุงูุฑ ุงู ฺฉ ุงู ูุณุฑู ุงูพู ุชููฺบ ุจูนูฺบ ุซูุงุ ููุฒ ุงูุฑ ููุฑ ฺฉ ูุฑุง ุฏ...\n",
      "[219/228] ฺฏูฺฏู ุฎุฑฺฏูุด\n",
      "   โ 17 paragraphs extracted\n",
      "   ๐ Preview: โูุฑ ุณูู ููู ุจฺู!  ฺฉุงู  ุงฺฉ ููฺพ ุณ ุฎุฑฺฏูุด ฺฉ ุฌุณ ฺฉุง ูุงู ฺฏูฺฏู ุชฺพุงู ุง...\n",
      "[220/228] ุนููููุฏ ฺูุง\n",
      "   โ 15 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ฺฺพููน ุณ ูพุงฺ ูฺบ ฺููฺบ ฺฉ ฺฉุฆ ุจู ุชฺพุ ุงุณ ูพุฑ ูพูุฏ ุจฺพ ูฺฏ ูุฆ ุชฺพูพูุฏูฺบ ฺฉ...\n",
      "[221/228] ุญูู ูุฌุฑู\n",
      "   โ 18 paragraphs extracted\n",
      "   ๐ Preview: ุงุณูู ุฑุงุช ุจฺพุฑ ุจุณุชุฑ ูพุฑ ฺฉุฑููนฺบ ุจุฏูุชุง ุฑุงุ ูฺฉู ููุฏ ุงุณ ุณ ฺฉูุณูฺบ ุฏูุฑ ุฑุขุฎุฑ ุฎุฏุง ุฎุฏุง...\n",
      "[222/228] ูุงูฺ ูฺฉฺพ\n",
      "   โ 15 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏูุน ฺฉุง ุฐฺฉุฑ  ุงฺฉ ฺูููน ุฌู ฺฉ ุฏุงู ุฌูุน ฺฉุฑู ฺฉ ูุฆ ุงฺฉ ุฑุณุช ุณ ฺฏุฐุฑ ุฑ ุชฺพ...\n",
      "[223/228] ููุช ูุตุญุช\n",
      "   โ 19 paragraphs extracted\n",
      "   ๐ Preview: ุณุฑ ุทูุญ ุงูุจ ุชุฑุจ ฺฉููน ฺฉ ูฺฏุฑุงฺบ ุชฺพ ฺฉููน ุจฺูฺบ ฺฉ ุงุฎูุงู ุชุฑุจุช ูุฒุฏ ุจุชุฑ ุจู...\n",
      "[224/228] ูุตุญุช ุจู ูุฌุงุช\n",
      "   โ 12 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุฏู ุงุญูุฏ ุงูุฑ ุงูู ฺฉ ุงู ุงุจู ู ุฌูฺฏู ูฺบ ุณุฑ ฺฉู ุฌุงู ฺฉุง ูพุฑูฺฏุฑุงู ุจูุงุงุงฺฏู ุฏู...\n",
      "[225/228] ุฎูุจุตูุฑุช ุฑุดุช\n",
      "   โ 13 paragraphs extracted\n",
      "   ๐ Preview:  ุจุงุช ุชู ุจุงุฑุง ุณู ุชฺพ ฺฉ ุชุงุฑุฎ ุงูพู ุขูพ ฺฉู ุฏุฑุงุช ุชุงู ุนูุฑ ุตุงุญุจ ฺฉู ุนูู ุทูุฑ ...\n",
      "[226/228] ุฑูุถุงู ุงูุฑ ุจฺ\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุฑูุถุงู ุงฺฉ ุงุณุง ูู  ุฌู ุจุชุฑ ุงูุณุงู ุงูุฑ ุจุชุฑ ูุณููุงู ุจูู ฺฉ ุดูู ฺฉู ูพุฑูุงู ฺฺฺพุงู...\n",
      "[227/228] ุญุงูุฏ ูุงฺบ ุงูุฑ ุฑูฺฏ ุจุฑูฺฏ ูพฺพูู\n",
      "   โ 11 paragraphs extracted\n",
      "   ๐ Preview: ูุฑ ุณูู ููู ุจฺู!  ฺฉุงู  ุงฺฉ ููฺพ ูู ุจฺ ุญุงูุฏ ูุงฺบ ฺฉ ุฌูฺบ ูพฺพูู ุจุช ูพ...\n",
      "[228/228] ุฒูุฏฺฏ ฺฉุง ุงูููู ุณุจู\n",
      "   โ 6 paragraphs extracted\n",
      "   ๐ Preview: ุงฺฉ ุขุฏู ฺฉ ฺุงุฑ ุจูน ุชฺพู ฺุงุชุง ุชฺพุง ฺฉ ุงุณ ฺฉ ุจูน  ุณุจู ุณฺฉฺพ ูฺบฺฉ ฺฉุณ ฺฉู ูพุฑ...\n",
      "\n",
      "======================================================================\n",
      "๐พ Saving to JSON...\n",
      "โ Saved 227 stories to: urdu_stories_clean.json\n",
      "๐ Statistics:\n",
      "   - Total stories: 227\n",
      "   - Failed stories: 0\n",
      "   - Total paragraphs: 3396\n",
      "   - Total words: 136,374\n",
      "   - Avg paragraphs/story: 15.0\n",
      "   - Avg words/story: 600.8\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_story_paragraphs(driver):\n",
    "    \"\"\"Extract ONLY story paragraphs using the specific XPath pattern\"\"\"\n",
    "    \n",
    "    paragraphs = []\n",
    "    \n",
    "    try:\n",
    "        parent_xpath = '//*[@id=\"main_content\"]/div[1]/div[2]/div[8]/div[2]'\n",
    "        \n",
    "        try:\n",
    "            story_container = driver.find_element(By.XPATH, parent_xpath)\n",
    "        except:\n",
    "            story_container = driver.find_element(By.XPATH, '//*[@id=\"main_content\"]//div[contains(@class, \"clear\")]')\n",
    "        \n",
    "        script = \"\"\"\n",
    "        var element = arguments[0];\n",
    "        var textNodes = [];\n",
    "        \n",
    "        for (var i = 0; i < element.childNodes.length; i++) {\n",
    "            var node = element.childNodes[i];\n",
    "            if (node.nodeType === Node.TEXT_NODE) {\n",
    "                var text = node.textContent.trim();\n",
    "                if (text.length > 20) {\n",
    "                    textNodes.push(text);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return textNodes;\n",
    "        \"\"\"\n",
    "        \n",
    "        text_nodes = driver.execute_script(script, story_container)\n",
    "        \n",
    "        for text in text_nodes:\n",
    "            if re.search(r'[\\u0600-\\u06FF]', text):\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                if len(text) > 20:\n",
    "                    paragraphs.append(text)\n",
    "        \n",
    "        return paragraphs\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      Error extracting paragraphs: {e}\")\n",
    "        return []\n",
    "\n",
    "def main():\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    # Set longer page load timeout\n",
    "    driver.set_page_load_timeout(30)  # 30 seconds timeout\n",
    "    \n",
    "    all_stories = []\n",
    "    story_links = []\n",
    "    \n",
    "    # Step 1: Loop through pages and collect story links\n",
    "    print(\"๐ Collecting story links from all pages...\\n\")\n",
    "    \n",
    "    for page_num in range(1, 21):\n",
    "        if page_num == 1:\n",
    "            url = \"https://www.urdupoint.com/kids/category/moral-stories.html\"\n",
    "        else:\n",
    "            url = f\"https://www.urdupoint.com/kids/category/moral-stories-page{page_num}.html\"\n",
    "        \n",
    "        print(f\"๐ Page {page_num}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            story_boxes = driver.find_elements(By.CSS_SELECTOR, 'a.sharp_box')\n",
    "            \n",
    "            page_count = 0\n",
    "            for box in story_boxes:\n",
    "                story_url = box.get_attribute('href')\n",
    "                urdu_title = box.find_element(By.CLASS_NAME, 'title_ur').text\n",
    "                \n",
    "                if story_url not in [s['url'] for s in story_links]:\n",
    "                    story_links.append({'title': urdu_title, 'url': story_url})\n",
    "                    page_count += 1\n",
    "            \n",
    "            print(f\"   โ Found {page_count} stories (Total: {len(story_links)})\\n\")\n",
    "        \n",
    "        except TimeoutException:\n",
    "            print(f\"   โ๏ธ Page load timeout - skipping page {page_num}\\n\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"   โ Error: {e}\\n\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"โ Total stories collected: {len(story_links)}\\n\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Step 2: Extract each story with retry logic\n",
    "    failed_stories = []\n",
    "    \n",
    "    for idx, story_info in enumerate(story_links, 1):\n",
    "        print(f\"[{idx}/{len(story_links)}] {story_info['title']}\")\n",
    "        \n",
    "        # Try up to 3 times for each story\n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "        \n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                driver.get(story_info['url'])\n",
    "                time.sleep(3)  # Increased delay\n",
    "                \n",
    "                wait = WebDriverWait(driver, 15)  # Increased wait time\n",
    "                wait.until(EC.presence_of_element_located((By.ID, \"main_content\")))\n",
    "                \n",
    "                try:\n",
    "                    title_elem = driver.find_element(By.CLASS_NAME, 'detail_heading')\n",
    "                    title = title_elem.text.strip()\n",
    "                except:\n",
    "                    title = story_info['title']\n",
    "                \n",
    "                paragraphs = extract_story_paragraphs(driver)\n",
    "                \n",
    "                if not paragraphs:\n",
    "                    print(f\"   โ๏ธ No story content extracted\")\n",
    "                    success = True  # Don't retry for empty content\n",
    "                    break\n",
    "                \n",
    "                full_text = '\\n\\n'.join(paragraphs)\n",
    "                \n",
    "                print(f\"   โ {len(paragraphs)} paragraphs extracted\")\n",
    "                \n",
    "                story_data = {\n",
    "                    'title': title,\n",
    "                    'paragraphs': paragraphs,\n",
    "                    'full_text': full_text\n",
    "                }\n",
    "                \n",
    "                all_stories.append(story_data)\n",
    "                \n",
    "                if paragraphs:\n",
    "                    preview = paragraphs[0][:80] + \"...\" if len(paragraphs[0]) > 80 else paragraphs[0]\n",
    "                    print(f\"   ๐ Preview: {preview}\")\n",
    "                \n",
    "                success = True\n",
    "                \n",
    "                # Longer delay between stories (be more polite)\n",
    "                time.sleep(3)\n",
    "            \n",
    "            except TimeoutException:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    print(f\"   โณ Timeout - Retry {retry_count}/{max_retries}\")\n",
    "                    time.sleep(5)  # Wait before retry\n",
    "                else:\n",
    "                    print(f\"   โ Timeout after {max_retries} retries - skipping\")\n",
    "                    failed_stories.append(story_info)\n",
    "            \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    print(f\"   โณ Error - Retry {retry_count}/{max_retries}: {e}\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"   โ Failed after {max_retries} retries: {e}\")\n",
    "                    failed_stories.append(story_info)\n",
    "        \n",
    "        # Restart driver every 50 stories to prevent memory issues\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"\\n   ๐ Restarting browser (processed {idx} stories)...\\n\")\n",
    "            driver.quit()\n",
    "            time.sleep(3)\n",
    "            driver = webdriver.Chrome()\n",
    "            driver.set_page_load_timeout(30)\n",
    "    \n",
    "    # Step 3: Save to JSON\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"๐พ Saving to JSON...\")\n",
    "    \n",
    "    output_file = 'urdu_stories_clean.json'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_stories, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    total_paragraphs = sum(len(story['paragraphs']) for story in all_stories)\n",
    "    total_words = sum(len(story['full_text'].split()) for story in all_stories)\n",
    "    \n",
    "    print(f\"โ Saved {len(all_stories)} stories to: {output_file}\")\n",
    "    print(f\"๐ Statistics:\")\n",
    "    print(f\"   - Total stories: {len(all_stories)}\")\n",
    "    print(f\"   - Failed stories: {len(failed_stories)}\")\n",
    "    print(f\"   - Total paragraphs: {total_paragraphs}\")\n",
    "    print(f\"   - Total words: {total_words:,}\")\n",
    "    if all_stories:\n",
    "        print(f\"   - Avg paragraphs/story: {total_paragraphs/len(all_stories):.1f}\")\n",
    "        print(f\"   - Avg words/story: {total_words/len(all_stories):.1f}\")\n",
    "    \n",
    "    # Save failed stories list\n",
    "    if failed_stories:\n",
    "        print(f\"\\nโ๏ธ Saving list of failed stories...\")\n",
    "        with open('failed_stories.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(failed_stories, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"   Saved to: failed_stories.json\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c627f7d",
   "metadata": {},
   "source": [
    "## Preprocessing adding EOP,EOS,EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0235612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ง Preprocessing Pipeline Started\n",
      "\n",
      "[1/227] ูพูุฑุงุณุฑุงุฑ ุจูฺฺพุง...\n",
      "[2/227] ูุง ุนุฒู...\n",
      "[3/227] ฺฏฺพุฑ ูฺบ ูุฌุฑู...\n",
      "[4/227] ุฌูุช ฺฉุง ุฑุงุณุช...\n",
      "[5/227] ุจฺพูุชูฺบ ฺฉุง ูุงู...\n",
      "[6/227] ุงูุงูุฏุงุฑ ฺฉุง ุงูุนุงู...\n",
      "[7/227] ุฑุญู ฺฉุง ุตู...\n",
      "[8/227] ุฑู ฺฉ ูุงูพุฑูุง...\n",
      "[9/227] ูุช...\n",
      "[10/227] ฺฉุฑู ูุฑุจุงู ุชู ุงูู ุฒูู ูพุฑ...\n",
      "[11/227] ูพุงูฺุง ุงูุฑ ุงุณูุงุฑูน ููู...\n",
      "[12/227] ููู ฺฉ ุฎูุงุด...\n",
      "[13/227] ุงูููู ุฏูุณุช...\n",
      "[14/227] ูุงู ฺฉุง ุณุงูุงู...\n",
      "[15/227] ุฎุงู ูพูุฌุฑ...\n",
      "[16/227] ุทุงูุชูุฑ ุจ ูููู...\n",
      "[17/227] ูฺฉฺ ุงุฑ ฺฉุง ุจูนุง...\n",
      "[18/227] ุงฺุฏ ฺฉุง ูุนุฏ...\n",
      "[19/227]  ุจฺฉุฑุง ฺฉุณ ฺฉุง ุ...\n",
      "[20/227] ูฺฉ ฺฉุง ุตู...\n",
      "[21/227] ูฺฏู...\n",
      "[22/227] ูพุฑ ฺฉ ููุชุง...\n",
      "[23/227] ุนูู ฺฉ ุงูุชุง ุฌุงูุช ...\n",
      "[24/227] ุณูพุฑ ุฑู...\n",
      "[25/227] ุชููุฑ ฺฉ ุฒูุฏฺฏ...\n",
      "[26/227] ฺฉูุง...\n",
      "[27/227] ฺฺพููน ุณ ุฎุจุฑ...\n",
      "[28/227] ุงูุนุงู...\n",
      "[29/227] ูฺฉ...\n",
      "[30/227] ุฏูุณุช ู ุฌู ูุตุจุช ูฺบ ฺฉุงู ุขุฆ...\n",
      "[31/227] ฺฺพููน ุณ ูฺฉ...\n",
      "[32/227] ุฏุงูุช ฺฉุง ุตู...\n",
      "[33/227] ูพุฑูุฏูฺบ ฺฉุง ูพุงฺฉุณุชุงู...\n",
      "[34/227] ุจุฑฺฏุฏ ฺฉุง ุฏุฑุฎุช...\n",
      "[35/227] ุนูู ุฎุฒุงู...\n",
      "[36/227] ูุงูฺ ฺฉุง ุงูุฌุงู...\n",
      "[37/227] ฺฉฺู ูพุช...\n",
      "[38/227] ุงุตู ูุญุณู...\n",
      "[39/227] ุฏูุณุฑุง ุฑุงุณุช...\n",
      "[40/227] ุขูุณูุคฺบ ฺฉ ุทุงูุช...\n",
      "[41/227] ุงุญูุฏ ฺฉ ฺฉุงู...\n",
      "[42/227] ุฌูฺฏู ูฺบ ูฺฏุงู...\n",
      "[43/227] ุฌุงุฏูุฆ ูุฑุบ...\n",
      "[44/227] ุนูุฑ ฺฉุง ุณุจู...\n",
      "[45/227] ูพูุฑุงุณุฑุงุฑ ูุงุฏ...\n",
      "[46/227] ุจุงุฏุฑ ุฑู ฺฉุง ุจฺ...\n",
      "[47/227] ฺฏฺู ุงูุฑ ุขู...\n",
      "[48/227] ูุญูุช ูฺฺฉุง...\n",
      "[49/227] ุจุฑููุช...\n",
      "[50/227] ุจุงุบ ฺฉ ุณุฑ...\n",
      "[51/227] ุงูุฏ ฺฉุง ฺุงูุฏ...\n",
      "[52/227] ูุดุงุฏ ุงูุฑ ฺฉุงู...\n",
      "[53/227] ุฌุงุฏูุฆ ฺฺพฺ...\n",
      "[54/227] ุงุญุณุงุณ...\n",
      "[55/227] ูุญูุช ฺฉุง ุตู...\n",
      "[56/227] ฺูฺฉ ูุงู ฺฉฺพููู...\n",
      "[57/227] ูพูุฏูฺบ ฺฉ ุงูุช...\n",
      "[58/227] ฺฉูููพู ฺฉ ฺฉุงู...\n",
      "[59/227] ฺฉู ุธุฑู...\n",
      "[60/227] ูุงูุฑูุงู ฺฉ ุณุฒุง...\n",
      "[61/227] ูุงุถ ฺฉุง ุงูุตุงู...\n",
      "[62/227] ูพูุฑ ุงุณุฑุงุฑ ุบุงุฑ...\n",
      "[63/227] ฺฉุงููนูฺบ ฺฉ ุฏุฑูุงู...\n",
      "[64/227] ุงููฺฉฺพ ุฏูุง...\n",
      "[65/227] ุดุงูุชู ุฌู...\n",
      "[66/227] ูฺบ ูพุงฺฉุณุชุงู ููุฌ ูฺบ...\n",
      "[67/227] ฺฏุฑู ฺฉุง ุณุจุจ...\n",
      "[68/227] ูฺฉ ฺฉุง ุจุฏู...\n",
      "[69/227] ูฺฉ ฺฉ ุชูุงุด...\n",
      "[70/227] ฺฉูู ุญููุงุฆ...\n",
      "[71/227] ุณฺฉููุณุชุงู...\n",
      "[72/227] ฺฺพููน ุณ ูฺฉฺ ุจฺ ุณ ุจุงุช...\n",
      "[73/227] ุณุงุช ุฑูฺฏ ฺฉุง ูพฺพูู...\n",
      "[74/227] ุณู ุจูุฑ ฺฏูุฏู...\n",
      "[75/227] ุงูุชุญุงู...\n",
      "[76/227] ูุณู...\n",
      "[77/227] ุฏุงุฑ ฺู ฺฉ ุขูุณู...\n",
      "[78/227] ฺูููนุงฺบ ุงฺฉ ุฏูุณุฑ ฺฉู ฺฉุณ ูพฺุงูุช ฺบ...\n",
      "[79/227] ุจ ุฒุจุงู ูุญุณู...\n",
      "[80/227] ุจููุชุง ุฌูฺฏู...\n",
      "[81/227] ุชุฑุจุช...\n",
      "[82/227] ฺฉุณ ุฑุ...\n",
      "[83/227] ูฺฉุงุฑ ุฌุงุฏูฺฏุฑู...\n",
      "[84/227] ุฎุฑฺฏูุด ูพูพู ฺฉุง ูุนุฏ...\n",
      "[85/227] ฺุงูุงฺฉ ูููฺ...\n",
      "[86/227] ุฎูุฏ ุบุฑุถ...\n",
      "[87/227] ฺูฺฏุงุฏฺ ฺฉ ฺุงูุงฺฉ...\n",
      "[88/227] ฺูุฑูฺบ ฺฉุง ุงูุฌุงู...\n",
      "[89/227] ููุงู ููุงุฒ...\n",
      "[90/227] ุจุฏ ฺฏูุงู...\n",
      "[91/227] ุดฺฉุฑ ฺฏุฒุงุฑ...\n",
      "[92/227] ุฒูุจ ู ฺฉฺพุฑ ูพฺฉุงุฆ...\n",
      "[93/227] ุฏุงูุง ุจุงุฏุดุง...\n",
      "[94/227] ุจุฑฺฏุฏ ุจูฺูฺฏ...\n",
      "[95/227] ูฺฉ ุฏู ุจุงุฏุดุง...\n",
      "[96/227] ุจุฏูุงู ุจูุฑุง...\n",
      "[97/227] ฺุงฺุง ุตุฏุฑ ุฏู...\n",
      "[98/227] ูุฑุง ูุดู...\n",
      "[99/227] ุจุงุฑุด ฺฉุง ููฺพุง ูุทุฑ...\n",
      "[100/227] ุฎูุงุจ ฺฉ ุชุนุจุฑ...\n",
      "[101/227] ูุฏ ฺฉ ุงุฐุช...\n",
      "[102/227] ุณฺ ุฏูุณุช...\n",
      "[103/227] ุดฺฉุฑ ูฺู...\n",
      "[104/227] ฺฏููุงู ุงูุฑ ููู ูพุฑ...\n",
      "[105/227] ุนูุงุฆ ู ฺุงุฆ ุจูุงุฆ...\n",
      "[106/227] ุงููฺฉฺพุง ุฌูฺฏู...\n",
      "[107/227] ุงูุฑ ูุนุงู ูู ฺฏุฆ...\n",
      "[108/227] ุขุฒุงุฏ  ุงฺฉ ูุนูุช...\n",
      "[109/227] ฺฺพููน ุณุฒุง...\n",
      "[110/227] ุจุงุฏุดุงุ ุดุฒุงุฏ ุงูุฑ ุดุฑุงุฑุช ุจฺพฺฺบ...\n",
      "[111/227] ฺฉู ุงูุฑ ุฎุฑฺฏูุด ฺฉ ุฏูุณุช...\n",
      "[112/227] ุจุงุฏุฑ ฺูพุงูู ุงูุฑ ุขู ฺฉุง ุฏุฑุฎุช...\n",
      "[113/227] ูุณูุช ุตุจุฑ ุงูุฑ ุฐุงูุช ฺฉ ฺฉุงู...\n",
      "[114/227] ุฌุงุฏูุฆ ุณฺฉ...\n",
      "[115/227] ูุง ููู ูพุฑุงู ุงุฏฺบ...\n",
      "[116/227] ุจ ูุตูุฑ...\n",
      "[117/227] ุงุชฺพ ูุฑุง ุณุงุชฺพ...\n",
      "[118/227] ฺฏุงุฆ ุงูุฑ ุจฺฉุฑ...\n",
      "[119/227] ุญุงุถุฑ ุฏูุงุบ ุจุงุฏุฑ...\n",
      "[120/227] ุชุชู ุณ ุฏูุณุช...\n",
      "[121/227] ูุงุฏุงู ฺฉุง ุงูุฌุงู...\n",
      "[122/227] ูุตุญุช ูพุฑ ุชูุฌ ุฏู...\n",
      "[123/227] ุฏู ุฌุงุณูุณ...\n",
      "[124/227] ูพุฑูุฏ ฺฉุง ุชุญู...\n",
      "[125/227] ุฑูฺฏุช...\n",
      "[126/227] ุงุตู ุฏููุช...\n",
      "[127/227] ฺุงุฑ ุนุฌุจ ุจูฺบ...\n",
      "[128/227] ุณุฑฺฉุณ ฺฉุง ฺฏฺพูฺุง...\n",
      "[129/227] ฺุชุงุ ูููฺ ุงูุฑ ฺฉุณุงู...\n",
      "[130/227] ุงูุฏฺพุง ุณุงููพ ุงูุฑ ฺุงฺฉู...\n",
      "[131/227] ุดุฏ ฺฉ ูฺฉฺพ ฺฉุง ฺฺพุช...\n",
      "[132/227] ุฏุงุฏุง ฺฉุง ฺฉฺพูููุง...\n",
      "[133/227] ุงฺฉ ุฏู ฺฉุง ุงุณุชุงุฏ...\n",
      "[134/227] ูฺบ ุงฺฉูุง ูฺบ ูฺบ...\n",
      "[135/227] ูพุฑุงูุง ุณฺฉ...\n",
      "[136/227] ุนููููุฏ ุจูุฏุฑ...\n",
      "[137/227] ูุนุฏ...\n",
      "[138/227] ุฑุณู ู ุฑูุงุช...\n",
      "[139/227] ูฺบ ุจุฒุฏู ูฺบ...\n",
      "[140/227] ฺฺพููน ุจฺ ุจฺุง ูุตู...\n",
      "[141/227] ูุชูุงุฒู ุบุฐุง ุถุฑูุฑ ...\n",
      "[142/227] ุงฺฉ ุชฺพ ุจฺฺพุง...\n",
      "[143/227] ูุงฺบ ฺฉ ุฏุนุง...\n",
      "[144/227] ุดุฑุงุฑุช ฺฉูุง ุงูุฑ ุฑุญู ุฏู ฺฺุง...\n",
      "[145/227] ุตุจุฑ ฺฉุง ุงูุนุงู...\n",
      "[146/227] ุงูู ุณุจ ฺฉุง ุฑุงุฒู...\n",
      "[147/227] ุฑูฺฏ ุจุฑูฺฏ ุชุชูุงฺบ...\n",
      "[148/227] ุชฺพุงูุณ ุงูุฑ ฺฉููุฑ...\n",
      "[149/227] ุงฺฉ ุชฺพ ูุฑูู...\n",
      "[150/227] ุณุงุช ูพุฑุงฺบ ุณุงุช ุณูุงฺบ...\n",
      "[151/227] ุฏูุณุช ู ุชู ุงุณ (ุขุฎุฑ ุญุต)...\n",
      "[152/227] ุฏูุณุช ู ุชู ุงุณ (ูพูุง ุญุต)...\n",
      "[153/227] ุดุฑุฑ ุจฺพุงูู...\n",
      "[154/227] ฺุงูุฏ ูพุฑ...\n",
      "[155/227] ุงฺฺพ ุจฺ ูฺบ ูฺุช...\n",
      "[156/227] ูุฏุฏ ฺฉุฑูุง ุณฺฉฺพู...\n",
      "[157/227] ุจููฺบ ฺฉุง ฺฏฺพุฑ...\n",
      "[158/227] ุงููฺฉฺพุง ุฎุท...\n",
      "[159/227] ุงุตู ุงูุฑ ูุณู ฺฉ ูพฺุงู...\n",
      "[160/227] ุฏูุณุช ู ุฌู ูุตุจุช ูฺบ ฺฉุงู ุขุฆ...\n",
      "[161/227] ูููุง ุจุฎุด...\n",
      "[162/227] ุฎุงู ูพูุฑ...\n",
      "[163/227] ฺูุฑ ฺฉูู...\n",
      "[164/227] ุบุฑูุฑ ฺฉ ุณุฒุง...\n",
      "[165/227] ฺฺพููนุง ูพฺพูู...\n",
      "[166/227] ูููู ฺฉ ุจุงุฏุฑ...\n",
      "[167/227] ฺูุจุช ฺฉู ุชูฺฉ ฺฉุง ุณุงุฑุง...\n",
      "[168/227] ุณุฒุง ฺฉุง ุจุฏู...\n",
      "[169/227] ุดุงู ฺฉุง ุจฺพููุง...\n",
      "[170/227] ุชุฐุจ ฺฉ ูพฺุงู...\n",
      "[171/227] ูุงูฺ ฺฉ ุณุฒุง...\n",
      "[172/227] ุงฺฉ ุงูุฑ ูููุน...\n",
      "[173/227] ุชุฏุงุจุฑ ุฑุงุญุชู ุงูุฒุง...\n",
      "[174/227] ุฌุงฺบ ุจ ูุจ ููุงุ ุฌุงู ูฺบ ุฌุงู ุขูุง...\n",
      "[175/227] ุขุฒุงุฏ...\n",
      "[176/227] ุณุฏุณ ฺฉุง ุจุงุบ...\n",
      "[177/227] ูููฺ ุงูุฑ ฺู ฺฉ ุฏูุณุช...\n",
      "[178/227] ุดุงู ุณ ูพู...\n",
      "[179/227] ุฐู ุฏุงุฑ ูนูฺฉู...\n",
      "[180/227] ฺูฺฉูุง ุฎุทุฑ...\n",
      "[181/227] ุฌูุฏ ุจุงุฒ ฺฉุง ุงูุฌุงู...\n",
      "[182/227] ฺฉูุงุช ุดุนุงุฑ...\n",
      "[183/227] ุงูพูุง ฺฉุงู ุฎูุฏ ฺฉุฑู...\n",
      "[184/227] ุชู ููุช...\n",
      "[185/227] ุฎูุงุดู ูพุฑูุงุฒ...\n",
      "[186/227] ุงูุณุงู ฺฉ ุนูุฑ...\n",
      "[187/227] ูุช ุจุฎุฑ...\n",
      "[188/227] ุฑุงุฆ...\n",
      "[189/227] ูพฺพูููฺบ ูุงูุง ุฑุงุณุช...\n",
      "[190/227] ูุงูุฑูุงู...\n",
      "[191/227] ุนููููุฏ ูฺฉฺุงุฑุง...\n",
      "[192/227] ูุฑ ูพุฑูุงุฒ...\n",
      "[193/227] ูุฒุฏูุฑ ูพุฑ ุธูู...\n",
      "[194/227] ุงุนุชูุงุฏ...\n",
      "[195/227] ุจฺพฺฺบ ุงูุฑ ุจฺพฺ...\n",
      "[196/227] ูุฌุจูุฑ ูุฌุฑู...\n",
      "[197/227] ูพูุฑุงุณุฑุงุฑ ูฺฺฉุง...\n",
      "[198/227] ุญูุงู ุฑุฒู ุงฺฉ ุนุจุงุฏุช...\n",
      "[199/227] ููู ฺฉู ูู ุฌฺพููน ฺฉ ุณุฒุง...\n",
      "[200/227] ููุช ฺฉ ุงูุช...\n",
      "[201/227] ุทูุท ฺฉ ุจุงุช...\n",
      "[202/227] ุฏูุณุช ู ุฌู ูุตุจุช ูฺบ ฺฉุงู ุขุฆ...\n",
      "[203/227] ุนูุงุจ ุงูุฑ ูฺฉฺ...\n",
      "[204/227] ุฎูุฏุฏุงุฑ ูฺฺฉ...\n",
      "[205/227] ุฎู ุจูนู...\n",
      "[206/227] ุงุญุณุงุณ...\n",
      "[207/227] ฺฉุงููนูฺบ ฺฉุง ุฌูุงุจ...\n",
      "[208/227] ุฑููน ุงูุฑ ููุณู...\n",
      "[209/227] ุชู ุจุงุฏุฑ...\n",
      "[210/227] ฺฉุงุง ูพููน...\n",
      "[211/227] ฺูู ุฎุงู...\n",
      "[212/227] ููุชูฺบ ฺฉุง ุงุฑ...\n",
      "[213/227] ุจููน ฺฉุงฺบ ฺฉฺพู ฺฏุง...\n",
      "[214/227] ุฌฺพู ูพ ุขุฆ ุงุชฺพ...\n",
      "[215/227] ุจูู ูุฑ ูฺฺพู (ุขุฎุฑ ุญุต)...\n",
      "[216/227] ุจูู ูุฑ ูฺฺพู (ูพูุง ุญุต)...\n",
      "[217/227] ููุช ฺฉุง ุฌูุงุจ...\n",
      "[218/227] ฺฏูฺฏู ุฎุฑฺฏูุด...\n",
      "[219/227] ุนููููุฏ ฺูุง...\n",
      "[220/227] ุญูู ูุฌุฑู...\n",
      "[221/227] ูุงูฺ ูฺฉฺพ...\n",
      "[222/227] ููุช ูุตุญุช...\n",
      "[223/227] ูุตุญุช ุจู ูุฌุงุช...\n",
      "[224/227] ุฎูุจุตูุฑุช ุฑุดุช...\n",
      "[225/227] ุฑูุถุงู ุงูุฑ ุจฺ...\n",
      "[226/227] ุญุงูุฏ ูุงฺบ ุงูุฑ ุฑูฺฏ ุจุฑูฺฏ ูพฺพูู...\n",
      "[227/227] ุฒูุฏฺฏ ฺฉุง ุงูููู ุณุจู...\n",
      "\n",
      "๐พ Saving preprocessed corpus...\n",
      "โ Preprocessed corpus saved!\n",
      "   Text file: preprocessed_corpus.txt (for tokenizer)\n",
      "   JSON file: preprocessed_stories.json (backup)\n",
      "\n",
      "๐ Final Statistics:\n",
      "   Stories: 227\n",
      "   <EOS> count: 10,810\n",
      "   <EOP> count: 3,396\n",
      "   <EOT> count: 227\n",
      "   Total chars: 720,300\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def full_preprocessing(input_file='urdu_stories_clean.json'):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        stories = json.load(f)\n",
    "    \n",
    "    print(\"๐ง Preprocessing Pipeline Started\\n\")\n",
    "    \n",
    "    # Special tokens\n",
    "    EOS = \" <EOS> \"\n",
    "    EOP = \" <EOP> \"\n",
    "    EOT = \" <EOT> \"\n",
    "    \n",
    "    processed_stories = []\n",
    "    \n",
    "    for idx, story in enumerate(stories, 1):\n",
    "        print(f\"[{idx}/{len(stories)}] {story['title'][:40]}...\")\n",
    "        \n",
    "        cleaned_paragraphs = []\n",
    "        \n",
    "        for paragraph in story['paragraphs']:\n",
    "            # Step 1: Remove extra whitespace\n",
    "            text = re.sub(r'\\s+', ' ', paragraph).strip()\n",
    "            \n",
    "            # Step 2: Normalize Unicode (NFC normalization)\n",
    "            text = unicodedata.normalize('NFC', text)\n",
    "            \n",
    "            # Step 3: Add <EOS> after sentences\n",
    "            # Split by Urdu sentence endings\n",
    "            sentences = re.split(r'([!?ุ])', text)\n",
    "            \n",
    "            processed_text = \"\"\n",
    "            for i in range(0, len(sentences)-1, 2):\n",
    "                sentence = sentences[i].strip()\n",
    "                punct = sentences[i+1] if i+1 < len(sentences) else \"\"\n",
    "                if sentence:\n",
    "                    processed_text += sentence + punct + EOS\n",
    "            \n",
    "            # Handle last part without punctuation\n",
    "            if len(sentences) % 2 == 1 and sentences[-1].strip():\n",
    "                processed_text += sentences[-1].strip() + EOS\n",
    "            \n",
    "            # Step 4: Add <EOP> after paragraph\n",
    "            processed_text = processed_text.strip() + EOP\n",
    "            \n",
    "            cleaned_paragraphs.append(processed_text)\n",
    "        \n",
    "        # Step 5: Combine and add <EOT>\n",
    "        story_text = \" \".join(cleaned_paragraphs) + EOT\n",
    "        \n",
    "        processed_stories.append({\n",
    "            'title': story['title'],\n",
    "            'text': story_text\n",
    "        })\n",
    "    \n",
    "    # Save for BPE tokenizer training\n",
    "    print(f\"\\n๐พ Saving preprocessed corpus...\")\n",
    "    \n",
    "    with open('preprocessed_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        for story in processed_stories:\n",
    "            f.write(story['text'] + \"\\n\\n\")\n",
    "    \n",
    "    # Also save as JSON (optional)\n",
    "    with open('preprocessed_stories.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_stories, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"โ Preprocessed corpus saved!\")\n",
    "    print(f\"   Text file: preprocessed_corpus.txt (for tokenizer)\")\n",
    "    print(f\"   JSON file: preprocessed_stories.json (backup)\")\n",
    "    \n",
    "    # Statistics\n",
    "    full_text = \" \".join([s['text'] for s in processed_stories])\n",
    "    \n",
    "    print(f\"\\n๐ Final Statistics:\")\n",
    "    print(f\"   Stories: {len(processed_stories)}\")\n",
    "    print(f\"   <EOS> count: {full_text.count('<EOS>'):,}\")\n",
    "    print(f\"   <EOP> count: {full_text.count('<EOP>'):,}\")\n",
    "    print(f\"   <EOT> count: {full_text.count('<EOT>'):,}\")\n",
    "    print(f\"   Total chars: {len(full_text):,}\")\n",
    "    \n",
    "    return processed_stories\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    full_preprocessing('urdu_stories_clean.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7864cb",
   "metadata": {},
   "source": [
    "## BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737d6a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "๐ค BPE TOKENIZER TRAINING\n",
      "======================================================================\n",
      "Target vocabulary size: 250\n",
      "\n",
      "๐ Loading corpus...\n",
      "โ Corpus loaded: 720,528 characters\n",
      "\n",
      "๐ง Building character-level vocabulary...\n",
      "โ Initial vocabulary: 112 unique characters\n",
      "\n",
      "๐ Learning BPE merges...\n",
      "\n",
      "  [10/139] Merged: ('ฺบ', '</w>') (freq: 10,699)\n",
      "  [20/139] Merged: ('ู', 'ฺบ</w>') (freq: 3,793)\n",
      "  [30/139] Merged: ('ู', 'ฺบ</w>') (freq: 3,032)\n",
      "  [40/139] Merged: ('', 'ู') (freq: 2,296)\n",
      "  [50/139] Merged: ('ุง', 'ู') (freq: 1,811)\n",
      "  [60/139] Merged: ('ุจฺพ', '</w>') (freq: 1,455)\n",
      "  [70/139] Merged: ('', '</w>') (freq: 1,173)\n",
      "  [80/139] Merged: ('ุฌ', 'ุง') (freq: 968)\n",
      "  [90/139] Merged: ('ุฎ', 'ู') (freq: 875)\n",
      "  [100/139] Merged: ('ุจ', '') (freq: 782)\n",
      "  [110/139] Merged: (':', 'โ') (freq: 687)\n",
      "  [120/139] Merged: ('ุจ', 'ฺ') (freq: 623)\n",
      "  [130/139] Merged: ('ุชฺพ', '</w>') (freq: 545)\n",
      "\n",
      "โ Completed 139 merges\n",
      "\n",
      "๐ Building final vocabulary...\n",
      "โ Final vocabulary size: 250\n",
      "โ Tokenizer saved to: bpe_tokenizer.json\n",
      "\n",
      "======================================================================\n",
      "๐งช Testing Tokenizer\n",
      "======================================================================\n",
      "\n",
      "Original: ุงฺฉ ุฏู ฺฉ ุจุงุช  <EOS>\n",
      "Tokens: [166, 66, 151, 137, 174, 131, 222, 119]\n",
      "Decoded: ุงฺฉ ุฏู ฺฉ ุจุงุช  <EOS>\n",
      "\n",
      "โ Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BPE Tokenizer - Saves to JSON\n",
    "Better for deployment and interoperability\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=250):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.id_to_token = {}\n",
    "    \n",
    "    def get_stats(self, words):\n",
    "        \"\"\"Count frequency of adjacent byte pairs\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word, freq in words.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i + 1]] += freq\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, words):\n",
    "        \"\"\"Merge the most frequent pair in vocabulary\"\"\"\n",
    "        new_words = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word in words:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_words[new_word] = words[word]\n",
    "        \n",
    "        return new_words\n",
    "    \n",
    "    def train(self, corpus_file):\n",
    "        \"\"\"Train BPE tokenizer on corpus\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"๐ค BPE TOKENIZER TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Target vocabulary size: {self.vocab_size}\\n\")\n",
    "        \n",
    "        # Load corpus\n",
    "        print(\"๐ Loading corpus...\")\n",
    "        with open(corpus_file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        print(f\"โ Corpus loaded: {len(text):,} characters\\n\")\n",
    "        \n",
    "        # Build initial vocabulary\n",
    "        print(\"๐ง Building character-level vocabulary...\")\n",
    "        \n",
    "        words = text.split()\n",
    "        word_freqs = Counter(words)\n",
    "        \n",
    "        # Initialize with character sequences\n",
    "        vocab = {}\n",
    "        for word, freq in word_freqs.items():\n",
    "            vocab[' '.join(list(word)) + ' </w>'] = freq\n",
    "        \n",
    "        print(f\"โ Initial vocabulary: {len(set(''.join(vocab.keys())))} unique characters\\n\")\n",
    "        \n",
    "        # Get base vocabulary\n",
    "        base_vocab = set()\n",
    "        for word in vocab.keys():\n",
    "            base_vocab.update(word.split())\n",
    "        \n",
    "        # Learn merges\n",
    "        print(\"๐ Learning BPE merges...\\n\")\n",
    "        \n",
    "        num_merges = self.vocab_size - len(base_vocab)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_stats(vocab)\n",
    "            \n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best_pair, vocab)\n",
    "            \n",
    "            # Store merge as string key for JSON compatibility\n",
    "            merge_key = f\"{best_pair[0]}|{best_pair[1]}\"\n",
    "            self.merges[merge_key] = ''.join(best_pair)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  [{i + 1}/{num_merges}] Merged: {best_pair} (freq: {pairs[best_pair]:,})\")\n",
    "        \n",
    "        print(f\"\\nโ Completed {len(self.merges)} merges\\n\")\n",
    "        \n",
    "        # Build final vocabulary\n",
    "        print(\"๐ Building final vocabulary...\")\n",
    "        \n",
    "        self.vocab = {char: idx for idx, char in enumerate(sorted(base_vocab))}\n",
    "        \n",
    "        current_idx = len(self.vocab)\n",
    "        for merge_key, merged in self.merges.items():\n",
    "            if merged not in self.vocab:\n",
    "                self.vocab[merged] = current_idx\n",
    "                current_idx += 1\n",
    "        \n",
    "        self.id_to_token = {idx: token for token, idx in self.vocab.items()}\n",
    "        \n",
    "        print(f\"โ Final vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        return self.vocab\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text using learned BPE\"\"\"\n",
    "        \n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply merges\n",
    "            while True:\n",
    "                pairs = [(word_tokens[i], word_tokens[i + 1]) \n",
    "                        for i in range(len(word_tokens) - 1)]\n",
    "                \n",
    "                bigram_to_merge = None\n",
    "                min_merge_idx = float('inf')\n",
    "                \n",
    "                for pair in pairs:\n",
    "                    merge_key = f\"{pair[0]}|{pair[1]}\"\n",
    "                    if merge_key in self.merges:\n",
    "                        merge_order = list(self.merges.keys()).index(merge_key)\n",
    "                        if merge_order < min_merge_idx:\n",
    "                            min_merge_idx = merge_order\n",
    "                            bigram_to_merge = pair\n",
    "                \n",
    "                if bigram_to_merge is None:\n",
    "                    break\n",
    "                \n",
    "                first, second = bigram_to_merge\n",
    "                new_token = first + second\n",
    "                new_word_tokens = []\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if i < len(word_tokens) - 1 and \\\n",
    "                       word_tokens[i] == first and word_tokens[i + 1] == second:\n",
    "                        new_word_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                \n",
    "                word_tokens = new_word_tokens\n",
    "            \n",
    "            # Convert to IDs\n",
    "            for token in word_tokens:\n",
    "                if token in self.vocab:\n",
    "                    tokens.append(self.vocab[token])\n",
    "                else:\n",
    "                    tokens.append(self.vocab.get('<UNK>', 0))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        \n",
    "        tokens = [self.id_to_token.get(idx, '<UNK>') for idx in token_ids]\n",
    "        text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def save(self, filepath='bpe_tokenizer.json'):\n",
    "        \"\"\"Save tokenizer to JSON file\"\"\"\n",
    "        \n",
    "        tokenizer_data = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'id_to_token': {str(k): v for k, v in self.id_to_token.items()},  # JSON keys must be strings\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(tokenizer_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"โ Tokenizer saved to: {filepath}\")\n",
    "    \n",
    "    def load(self, filepath='bpe_tokenizer.json'):\n",
    "        \"\"\"Load tokenizer from JSON file\"\"\"\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            tokenizer_data = json.load(f)\n",
    "        \n",
    "        self.vocab = tokenizer_data['vocab']\n",
    "        self.merges = tokenizer_data['merges']\n",
    "        self.id_to_token = {int(k): v for k, v in tokenizer_data['id_to_token'].items()}  # Convert back to int\n",
    "        self.vocab_size = tokenizer_data['vocab_size']\n",
    "        \n",
    "        print(f\"โ Tokenizer loaded from: {filepath}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Train tokenizer\n",
    "    tokenizer = BPETokenizer(vocab_size=250)\n",
    "    tokenizer.train('preprocessed_corpus.txt')\n",
    "    \n",
    "    # Save as JSON\n",
    "    tokenizer.save('bpe_tokenizer.json')\n",
    "    \n",
    "    # Test\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"๐งช Testing Tokenizer\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    test_text = \"ุงฺฉ ุฏู ฺฉ ุจุงุช  <EOS>\"\n",
    "    print(f\"Original: {test_text}\")\n",
    "    \n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "    \n",
    "    print(\"\\nโ Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BPE Tokenizer - Loads from JSON\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.id_to_token = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def load(self, filepath='bpe_tokenizer.json'):\n",
    "        \"\"\"Load tokenizer from JSON file\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.vocab = data['vocab']\n",
    "        self.merges = data['merges']\n",
    "        self.id_to_token = {int(k): v for k, v in data['id_to_token'].items()}\n",
    "        self.vocab_size = data['vocab_size']\n",
    "        \n",
    "        print(f\"โ Tokenizer loaded: vocab size = {len(self.vocab)}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text into token IDs\"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply merges\n",
    "            while True:\n",
    "                pairs = [(word_tokens[i], word_tokens[i + 1]) \n",
    "                        for i in range(len(word_tokens) - 1)]\n",
    "                \n",
    "                bigram_to_merge = None\n",
    "                min_merge_idx = float('inf')\n",
    "                \n",
    "                for pair in pairs:\n",
    "                    merge_key = f\"{pair[0]}|{pair[1]}\"\n",
    "                    if merge_key in self.merges:\n",
    "                        merge_order = list(self.merges.keys()).index(merge_key)\n",
    "                        if merge_order < min_merge_idx:\n",
    "                            min_merge_idx = merge_order\n",
    "                            bigram_to_merge = pair\n",
    "                \n",
    "                if bigram_to_merge is None:\n",
    "                    break\n",
    "                \n",
    "                first, second = bigram_to_merge\n",
    "                new_token = first + second\n",
    "                new_word_tokens = []\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if i < len(word_tokens) - 1 and \\\n",
    "                       word_tokens[i] == first and word_tokens[i + 1] == second:\n",
    "                        new_word_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                \n",
    "                word_tokens = new_word_tokens\n",
    "            \n",
    "            # Convert to IDs\n",
    "            for token in word_tokens:\n",
    "                if token in self.vocab:\n",
    "                    tokens.append(self.vocab[token])\n",
    "                else:\n",
    "                    tokens.append(self.vocab.get('<UNK>', 0))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        tokens = [self.id_to_token.get(idx, '<UNK>') for idx in token_ids]\n",
    "        text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b9af6",
   "metadata": {},
   "source": [
    "## Trigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c66a778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "๐ค Loading tokenizer...\n",
      "โ Tokenizer loaded from: bpe_tokenizer.json\n",
      "\n",
      "๐ Loading and tokenizing corpus...\n",
      "โ Tokenized: 353,464 tokens\n",
      "\n",
      "======================================================================\n",
      "๐ TRIGRAM MODEL TRAINING\n",
      "======================================================================\n",
      "Interpolation: ฮป1=0.1, ฮป2=0.3, ฮป3=0.6\n",
      "\n",
      "๐ Training on 353,464 tokens...\n",
      "\n",
      "๐ข Counting n-grams...\n",
      "โ Unigrams: 243\n",
      "โ Bigrams: 11,777\n",
      "โ Trigrams: 85,206\n",
      "\n",
      "โ Training complete!\n",
      "โ Model saved to: trigram_model.json\n",
      "\n",
      "======================================================================\n",
      "๐งช Testing Generation\n",
      "======================================================================\n",
      "\n",
      "Prefix: ุงฺฉ ุฏู\n",
      "Generated:\n",
      "ุงฺฉ ุฏู <UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK><UNK>\n",
      "\n",
      "โ Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trigram Language Model - Saves to JSON\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "    def __init__(self, lambda1=0.1, lambda2=0.3, lambda3=0.6):\n",
    "        assert abs(lambda1 + lambda2 + lambda3 - 1.0) < 1e-6\n",
    "        \n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.lambda3 = lambda3\n",
    "        \n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.trigram_counts = defaultdict(Counter)\n",
    "        \n",
    "        self.total_tokens = 0\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.EOS = '<EOS>'\n",
    "        self.EOP = '<EOP>'\n",
    "        self.EOT = '<EOT>'\n",
    "        self.START = '<START>'\n",
    "    \n",
    "    def train(self, tokenized_corpus):\n",
    "        \"\"\"Train trigram model\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"๐ TRIGRAM MODEL TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Interpolation: ฮป1={self.lambda1}, ฮป2={self.lambda2}, ฮป3={self.lambda3}\\n\")\n",
    "        \n",
    "        print(f\"๐ Training on {len(tokenized_corpus):,} tokens...\\n\")\n",
    "        \n",
    "        # Add start tokens\n",
    "        tokens = [self.START, self.START] + tokenized_corpus\n",
    "        \n",
    "        self.total_tokens = len(tokens)\n",
    "        self.vocab = set(tokens)\n",
    "        \n",
    "        print(\"๐ข Counting n-grams...\")\n",
    "        \n",
    "        # Count unigrams\n",
    "        for token in tokens:\n",
    "            self.unigram_counts[token] += 1\n",
    "        \n",
    "        # Count bigrams\n",
    "        for i in range(len(tokens) - 1):\n",
    "            w1 = tokens[i]\n",
    "            w2 = tokens[i + 1]\n",
    "            self.bigram_counts[w1][w2] += 1\n",
    "        \n",
    "        # Count trigrams\n",
    "        for i in range(len(tokens) - 2):\n",
    "            w1 = tokens[i]\n",
    "            w2 = tokens[i + 1]\n",
    "            w3 = tokens[i + 2]\n",
    "            self.trigram_counts[(w1, w2)][w3] += 1\n",
    "        \n",
    "        print(f\"โ Unigrams: {len(self.unigram_counts):,}\")\n",
    "        print(f\"โ Bigrams: {sum(len(v) for v in self.bigram_counts.values()):,}\")\n",
    "        print(f\"โ Trigrams: {sum(len(v) for v in self.trigram_counts.values()):,}\")\n",
    "        print(\"\\nโ Training complete!\")\n",
    "    \n",
    "    def get_probability(self, w1, w2, w3):\n",
    "        \"\"\"Calculate interpolated probability\"\"\"\n",
    "        \n",
    "        # Unigram\n",
    "        p_unigram = self.unigram_counts[w3] / self.total_tokens\n",
    "        \n",
    "        # Bigram\n",
    "        if self.unigram_counts[w2] > 0:\n",
    "            p_bigram = self.bigram_counts[w2][w3] / self.unigram_counts[w2]\n",
    "        else:\n",
    "            p_bigram = 0\n",
    "        \n",
    "        # Trigram\n",
    "        if self.bigram_counts[w1][w2] > 0:\n",
    "            p_trigram = self.trigram_counts[(w1, w2)][w3] / self.bigram_counts[w1][w2]\n",
    "        else:\n",
    "            p_trigram = 0\n",
    "        \n",
    "        # Interpolated\n",
    "        p_interpolated = (\n",
    "            self.lambda1 * p_unigram +\n",
    "            self.lambda2 * p_bigram +\n",
    "            self.lambda3 * p_trigram\n",
    "        )\n",
    "        \n",
    "        return p_interpolated\n",
    "    \n",
    "    def generate(self, prefix_tokens=None, max_length=500, temperature=1.0):\n",
    "        \"\"\"Generate text\"\"\"\n",
    "        \n",
    "        if prefix_tokens is None or len(prefix_tokens) < 2:\n",
    "            generated = [self.START, self.START]\n",
    "        else:\n",
    "            generated = list(prefix_tokens)\n",
    "        \n",
    "        if len(generated) < 2:\n",
    "            generated = [self.START] + generated\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            w1 = generated[-2]\n",
    "            w2 = generated[-1]\n",
    "            \n",
    "            candidates = self.vocab\n",
    "            probabilities = []\n",
    "            valid_candidates = []\n",
    "            \n",
    "            for candidate in candidates:\n",
    "                prob = self.get_probability(w1, w2, candidate)\n",
    "                if prob > 0:\n",
    "                    probabilities.append(prob)\n",
    "                    valid_candidates.append(candidate)\n",
    "            \n",
    "            if not valid_candidates:\n",
    "                break\n",
    "            \n",
    "            # Temperature sampling\n",
    "            if temperature != 1.0:\n",
    "                probabilities = np.array(probabilities)\n",
    "                probabilities = np.power(probabilities, 1.0 / temperature)\n",
    "                probabilities = probabilities / probabilities.sum()\n",
    "            else:\n",
    "                total = sum(probabilities)\n",
    "                probabilities = [p / total for p in probabilities]\n",
    "            \n",
    "            next_token = np.random.choice(valid_candidates, p=probabilities)\n",
    "            generated.append(next_token)\n",
    "            \n",
    "            if next_token == self.EOT:\n",
    "                break\n",
    "        \n",
    "        generated = [t for t in generated if t != self.START]\n",
    "        \n",
    "        return generated\n",
    "    \n",
    "    def save(self, filepath='trigram_model.json'):\n",
    "        \"\"\"Save model to JSON\"\"\"\n",
    "        \n",
    "        # Convert to JSON-serializable format\n",
    "        model_data = {\n",
    "            'lambda1': self.lambda1,\n",
    "            'lambda2': self.lambda2,\n",
    "            'lambda3': self.lambda3,\n",
    "            'unigram_counts': dict(self.unigram_counts),\n",
    "            'bigram_counts': {str(k): dict(v) for k, v in self.bigram_counts.items()},\n",
    "            'trigram_counts': {f\"{k[0]}|{k[1]}\": dict(v) for k, v in self.trigram_counts.items()},\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'vocab': list(self.vocab)\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"โ Model saved to: {filepath}\")\n",
    "    \n",
    "    def load(self, filepath='trigram_model.json'):\n",
    "        \"\"\"Load model from JSON\"\"\"\n",
    "        \n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "        \n",
    "        self.lambda1 = model_data['lambda1']\n",
    "        self.lambda2 = model_data['lambda2']\n",
    "        self.lambda3 = model_data['lambda3']\n",
    "        self.unigram_counts = Counter(model_data['unigram_counts'])\n",
    "        self.bigram_counts = defaultdict(Counter,\n",
    "                                        {k: Counter(v) for k, v in model_data['bigram_counts'].items()})\n",
    "        \n",
    "        # Convert trigram keys back\n",
    "        trigram_data = {}\n",
    "        for key, value in model_data['trigram_counts'].items():\n",
    "            w1, w2 = key.split('|')\n",
    "            trigram_data[(w1, w2)] = Counter(value)\n",
    "        self.trigram_counts = defaultdict(Counter, trigram_data)\n",
    "        \n",
    "        self.total_tokens = model_data['total_tokens']\n",
    "        self.vocab = set(model_data['vocab'])\n",
    "        \n",
    "        print(f\"โ Model loaded from: {filepath}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load tokenizer\n",
    "    print(\"๐ค Loading tokenizer...\")\n",
    "    tokenizer = BPETokenizer()\n",
    "    tokenizer.load('bpe_tokenizer.json')\n",
    "    \n",
    "    # Tokenize corpus\n",
    "    print(\"\\n๐ Loading and tokenizing corpus...\")\n",
    "    with open('preprocessed_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "        corpus = f.read()\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(corpus)\n",
    "    print(f\"โ Tokenized: {len(tokenized):,} tokens\\n\")\n",
    "    \n",
    "    # Train model\n",
    "    model = TrigramLanguageModel(lambda1=0.1, lambda2=0.3, lambda3=0.6)\n",
    "    model.train(tokenized)\n",
    "    \n",
    "    # Save as JSON\n",
    "    model.save('trigram_model.json')\n",
    "    \n",
    "    # Test\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"๐งช Testing Generation\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    prefix = \"ุงฺฉ ุฏู\"\n",
    "    prefix_tokens = tokenizer.tokenize(prefix)\n",
    "    \n",
    "    print(f\"Prefix: {prefix}\")\n",
    "    generated = model.generate(prefix_tokens, max_length=50)\n",
    "    text = tokenizer.decode(generated)\n",
    "    \n",
    "    print(f\"Generated:\\n{text}\")\n",
    "    \n",
    "    print(\"\\nโ Done!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5622614",
   "metadata": {},
   "source": [
    "## testing bpe and trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00eaef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "๐ฏ COMPLETE MODEL VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "๐งช TESTING PREPROCESSED CORPUS\n",
      "======================================================================\n",
      "\n",
      "๐ Special Token Counts:\n",
      "   <EOS> (End of Sentence): 10,810\n",
      "   <EOP> (End of Paragraph): 3,396\n",
      "   <EOT> (End of Text/Story): 227\n",
      "\n",
      "๐ Estimated stories in corpus: 227\n",
      "\n",
      "๐ Content Analysis:\n",
      "   Total characters: 720,528\n",
      "   Urdu characters: 481,695\n",
      "   Urdu content: 66.9%\n",
      "   โ๏ธ Low Urdu content (might have too many English characters)\n",
      "\n",
      "โ Corpus verification complete!\n",
      "\n",
      "======================================================================\n",
      "๐งช TESTING BPE TOKENIZER\n",
      "======================================================================\n",
      "โ Tokenizer loaded\n",
      "   Vocabulary size: 250\n",
      "   Merges learned: 139\n",
      "\n",
      "๐ Test 1: Basic Tokenization\n",
      "   Input: ุงฺฉ ุฏู ฺฉ ุจุงุช \n",
      "   Token IDs: [166, 66, 151, 137, 174, 131, 222]\n",
      "   Number of tokens: 7\n",
      "   Decoded: ุงฺฉ ุฏู ฺฉ ุจุงุช \n",
      "   โ PASS: Decoding matches original\n",
      "\n",
      "๐ Test 2: Special Tokens\n",
      "   Input: ฺฉุงู ุฎุชู ูุฆ <EOS> <EOP> <EOT>\n",
      "   Token IDs: [97, 101, 126, 112, 65, 243, 150, 172, 119, 136, 114, 29, 115]\n",
      "   Decoded: ฺฉุงู ุฎุชู ูุฆ <EOS> <EOP> <EOT>\n",
      "   โ PASS: Special tokens preserved\n",
      "\n",
      "๐ Test 3: Vocabulary Coverage\n",
      "   Urdu characters in vocab: 8/8\n",
      "   โ Special tokens found in vocabulary\n",
      "\n",
      "โ Tokenizer tests complete!\n",
      "\n",
      "======================================================================\n",
      "๐งช TESTING TRIGRAM MODEL\n",
      "======================================================================\n",
      "\n",
      "๐ Loading trigram model...\n",
      "โ Model loaded\n",
      "   Lambda values: ฮป1=0.1, ฮป2=0.3, ฮป3=0.6\n",
      "   Unigram tokens: 243\n",
      "   Bigram contexts: 243\n",
      "   Trigram contexts: 11,777\n",
      "   Total tokens trained on: 353,466\n",
      "   โ Lambda values sum to 1.0\n",
      "\n",
      "โ Trigram model tests complete!\n",
      "\n",
      "======================================================================\n",
      "โ ALL TESTS PASSED!\n",
      "======================================================================\n",
      "\n",
      "๐ Summary:\n",
      "   โ Preprocessed corpus has special tokens\n",
      "   โ BPE tokenizer trained (vocab size: 250)\n",
      "   โ Trigram model trained with interpolation\n",
      "\n",
      "๐ Your models are ready for deployment!\n",
      "\n",
      "๐ Next steps:\n",
      "   1. Create FastAPI backend (Phase IV)\n",
      "   2. Build web frontend (Phase V)\n",
      "   3. Deploy to Vercel (Phase VI)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test Script - Verify BPE Tokenizer and Trigram Model\n",
    "This will test if your trained models work correctly\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.id_to_token = {}\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def load(self, filepath='bpe_tokenizer.json'):\n",
    "        \"\"\"Load tokenizer from JSON\"\"\"\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.vocab = data['vocab']\n",
    "        self.merges = data['merges']\n",
    "        self.id_to_token = {int(k): v for k, v in data['id_to_token'].items()}\n",
    "        self.vocab_size = data['vocab_size']\n",
    "        \n",
    "        print(f\"โ Tokenizer loaded\")\n",
    "        print(f\"   Vocabulary size: {len(self.vocab)}\")\n",
    "        print(f\"   Merges learned: {len(self.merges)}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text\"\"\"\n",
    "        words = text.split()\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply merges\n",
    "            while True:\n",
    "                pairs = [(word_tokens[i], word_tokens[i + 1]) \n",
    "                        for i in range(len(word_tokens) - 1)]\n",
    "                \n",
    "                bigram_to_merge = None\n",
    "                min_merge_idx = float('inf')\n",
    "                \n",
    "                for pair in pairs:\n",
    "                    merge_key = f\"{pair[0]}|{pair[1]}\"\n",
    "                    if merge_key in self.merges:\n",
    "                        merge_order = list(self.merges.keys()).index(merge_key)\n",
    "                        if merge_order < min_merge_idx:\n",
    "                            min_merge_idx = merge_order\n",
    "                            bigram_to_merge = pair\n",
    "                \n",
    "                if bigram_to_merge is None:\n",
    "                    break\n",
    "                \n",
    "                first, second = bigram_to_merge\n",
    "                new_token = first + second\n",
    "                new_word_tokens = []\n",
    "                \n",
    "                i = 0\n",
    "                while i < len(word_tokens):\n",
    "                    if i < len(word_tokens) - 1 and \\\n",
    "                       word_tokens[i] == first and word_tokens[i + 1] == second:\n",
    "                        new_word_tokens.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word_tokens.append(word_tokens[i])\n",
    "                        i += 1\n",
    "                \n",
    "                word_tokens = new_word_tokens\n",
    "            \n",
    "            # Convert to IDs\n",
    "            for token in word_tokens:\n",
    "                if token in self.vocab:\n",
    "                    tokens.append(self.vocab[token])\n",
    "                else:\n",
    "                    # Unknown token\n",
    "                    tokens.append(self.vocab.get('<UNK>', 0))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text\"\"\"\n",
    "        tokens = [self.id_to_token.get(idx, '<UNK>') for idx in token_ids]\n",
    "        text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "        return text\n",
    "\n",
    "\n",
    "def test_tokenizer():\n",
    "    \"\"\"Test the BPE tokenizer\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"๐งช TESTING BPE TOKENIZER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = BPETokenizer()\n",
    "    tokenizer.load('bpe_tokenizer.json')\n",
    "    \n",
    "    # Test 1: Basic tokenization\n",
    "    print(\"\\n๐ Test 1: Basic Tokenization\")\n",
    "    test_text = \"ุงฺฉ ุฏู ฺฉ ุจุงุช \"\n",
    "    \n",
    "    print(f\"   Input: {test_text}\")\n",
    "    \n",
    "    tokens = tokenizer.tokenize(test_text)\n",
    "    print(f\"   Token IDs: {tokens}\")\n",
    "    print(f\"   Number of tokens: {len(tokens)}\")\n",
    "    \n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"   Decoded: {decoded}\")\n",
    "    \n",
    "    if test_text == decoded:\n",
    "        print(\"   โ PASS: Decoding matches original\")\n",
    "    else:\n",
    "        print(\"   โ๏ธ WARN: Decoding differs slightly (normal for some tokens)\")\n",
    "    \n",
    "    # Test 2: Special tokens\n",
    "    print(\"\\n๐ Test 2: Special Tokens\")\n",
    "    test_with_special = \"ฺฉุงู ุฎุชู ูุฆ <EOS> <EOP> <EOT>\"\n",
    "    \n",
    "    print(f\"   Input: {test_with_special}\")\n",
    "    \n",
    "    tokens = tokenizer.tokenize(test_with_special)\n",
    "    print(f\"   Token IDs: {tokens}\")\n",
    "    \n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"   Decoded: {decoded}\")\n",
    "    \n",
    "    if \"<EOS>\" in decoded or \"<EOP>\" in decoded or \"<EOT>\" in decoded:\n",
    "        print(\"   โ PASS: Special tokens preserved\")\n",
    "    else:\n",
    "        print(\"   โ๏ธ WARN: Special tokens not fully preserved\")\n",
    "    \n",
    "    # Test 3: Vocabulary coverage\n",
    "    print(\"\\n๐ Test 3: Vocabulary Coverage\")\n",
    "    \n",
    "    # Check for important Urdu characters\n",
    "    urdu_chars = ['ุง', 'ุจ', 'ุช', 'ฺฉ', 'ู', 'ู', '', '']\n",
    "    found = sum(1 for char in urdu_chars if char in tokenizer.vocab)\n",
    "    \n",
    "    print(f\"   Urdu characters in vocab: {found}/{len(urdu_chars)}\")\n",
    "    \n",
    "    # Check for special tokens\n",
    "    special_tokens = ['<EOS>', '<EOP>', '<EOT>']\n",
    "    has_special = any(token in str(tokenizer.vocab.keys()) for token in special_tokens)\n",
    "    \n",
    "    if has_special:\n",
    "        print(f\"   โ Special tokens found in vocabulary\")\n",
    "    else:\n",
    "        print(f\"   โ๏ธ Special tokens not in vocabulary\")\n",
    "    \n",
    "    print(\"\\nโ Tokenizer tests complete!\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def test_trigram_model():\n",
    "    \"\"\"Test the trigram model\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"๐งช TESTING TRIGRAM MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"\\n๐ Loading trigram model...\")\n",
    "    \n",
    "    with open('trigram_model.json', 'r', encoding='utf-8') as f:\n",
    "        model_data = json.load(f)\n",
    "    \n",
    "    print(f\"โ Model loaded\")\n",
    "    print(f\"   Lambda values: ฮป1={model_data['lambda1']}, ฮป2={model_data['lambda2']}, ฮป3={model_data['lambda3']}\")\n",
    "    \n",
    "    # Check unigrams\n",
    "    unigram_count = len(model_data['unigram_counts'])\n",
    "    print(f\"   Unigram tokens: {unigram_count:,}\")\n",
    "    \n",
    "    # Check bigrams\n",
    "    bigram_count = len(model_data['bigram_counts'])\n",
    "    print(f\"   Bigram contexts: {bigram_count:,}\")\n",
    "    \n",
    "    # Check trigrams\n",
    "    trigram_count = len(model_data['trigram_counts'])\n",
    "    print(f\"   Trigram contexts: {trigram_count:,}\")\n",
    "    \n",
    "    # Check total tokens\n",
    "    total_tokens = model_data['total_tokens']\n",
    "    print(f\"   Total tokens trained on: {total_tokens:,}\")\n",
    "    \n",
    "    # Verify lambdas sum to 1\n",
    "    lambda_sum = model_data['lambda1'] + model_data['lambda2'] + model_data['lambda3']\n",
    "    \n",
    "    if abs(lambda_sum - 1.0) < 0.001:\n",
    "        print(f\"   โ Lambda values sum to 1.0\")\n",
    "    else:\n",
    "        print(f\"   โ ERROR: Lambda values sum to {lambda_sum}\")\n",
    "    \n",
    "    print(\"\\nโ Trigram model tests complete!\")\n",
    "\n",
    "\n",
    "def verify_corpus():\n",
    "    \"\"\"Verify the preprocessed corpus\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"๐งช TESTING PREPROCESSED CORPUS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open('preprocessed_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "        corpus = f.read()\n",
    "    \n",
    "    # Count special tokens\n",
    "    eos_count = corpus.count('<EOS>')\n",
    "    eop_count = corpus.count('<EOP>')\n",
    "    eot_count = corpus.count('<EOT>')\n",
    "    \n",
    "    print(f\"\\n๐ Special Token Counts:\")\n",
    "    print(f\"   <EOS> (End of Sentence): {eos_count:,}\")\n",
    "    print(f\"   <EOP> (End of Paragraph): {eop_count:,}\")\n",
    "    print(f\"   <EOT> (End of Text/Story): {eot_count:,}\")\n",
    "    \n",
    "    # Estimate number of stories\n",
    "    print(f\"\\n๐ Estimated stories in corpus: {eot_count}\")\n",
    "    \n",
    "    # Check Urdu content\n",
    "    import re\n",
    "    urdu_chars = len(re.findall(r'[\\u0600-\\u06FF]', corpus))\n",
    "    total_chars = len(corpus)\n",
    "    \n",
    "    urdu_percentage = (urdu_chars / total_chars) * 100\n",
    "    \n",
    "    print(f\"\\n๐ Content Analysis:\")\n",
    "    print(f\"   Total characters: {total_chars:,}\")\n",
    "    print(f\"   Urdu characters: {urdu_chars:,}\")\n",
    "    print(f\"   Urdu content: {urdu_percentage:.1f}%\")\n",
    "    \n",
    "    if urdu_percentage > 70:\n",
    "        print(f\"   โ Good Urdu content coverage\")\n",
    "    else:\n",
    "        print(f\"   โ๏ธ Low Urdu content (might have too many English characters)\")\n",
    "    \n",
    "    print(\"\\nโ Corpus verification complete!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all tests\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"๐ฏ COMPLETE MODEL VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Corpus\n",
    "        verify_corpus()\n",
    "        \n",
    "        # Test 2: Tokenizer\n",
    "        tokenizer = test_tokenizer()\n",
    "        \n",
    "        # Test 3: Trigram Model\n",
    "        test_trigram_model()\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"โ ALL TESTS PASSED!\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"\\n๐ Summary:\")\n",
    "        print(\"   โ Preprocessed corpus has special tokens\")\n",
    "        print(\"   โ BPE tokenizer trained (vocab size: 250)\")\n",
    "        print(\"   โ Trigram model trained with interpolation\")\n",
    "        print(\"\\n๐ Your models are ready for deployment!\")\n",
    "        print(\"\\n๐ Next steps:\")\n",
    "        print(\"   1. Create FastAPI backend (Phase IV)\")\n",
    "        print(\"   2. Build web frontend (Phase V)\")\n",
    "        print(\"   3. Deploy to Vercel (Phase VI)\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nโ ERROR: File not found - {e}\")\n",
    "        print(\"Make sure these files are in the same directory:\")\n",
    "        print(\"   - bpe_tokenizer.json\")\n",
    "        print(\"   - trigram_model.json\")\n",
    "        print(\"   - preprocessed_corpus.txt\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nโ ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
